{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMetFU58OOCfmcXL+YfyGYR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sb-iam/cot-dfa/blob/main/notebooks/cot_dfa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooB2PE-s0QUM"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "     CELL 0: CoT-DFA - CHAIN-OF-THOUGHT DATAFLOW ANALYSIS\n",
        "     Applying Compiler Reaching Definitions to Detect Unfaithful Reasoning\n",
        "================================================================================\n",
        "\n",
        "MATS 10.0 APPLICATION PROJECT FOR NEEL NANDA\n",
        "\n",
        "  \"Can we tell when a CoT was causally important for giving its answer?\"\n",
        "                                        — Neel Nanda, MATS 10.0 Interests\n",
        "\n",
        "================================================================================\n",
        "                         CORE RESEARCH QUESTION\n",
        "================================================================================\n",
        "\n",
        "  Can compiler-style reaching definitions analysis detect unfaithful\n",
        "  Chain-of-Thought in code generation models?\n",
        "\n",
        "  ┌─────────────────────────────────────────────────────────────────────────┐\n",
        "  │  COMPILER DATAFLOW ANALYSIS    ←──BRIDGE──→    COT FAITHFULNESS        │\n",
        "  │  ─────────────────────────────                 ────────────────────────  │\n",
        "  │  • Reaching definitions         STRUCTURAL     • Which CoT matters?     │\n",
        "  │  • Dead code elimination        FAITHFULNESS   • Post-hoc rationalization│\n",
        "  │  • Use-def chains               METRICS        • Phantom code detection │\n",
        "  │  • O(1) single-pass analysis                   • No model calls needed  │\n",
        "  └─────────────────────────────────────────────────────────────────────────┘\n",
        "\n",
        "================================================================================\n",
        "                         PRIMARY HYPOTHESIS\n",
        "================================================================================\n",
        "\n",
        "  H₁: phantom_ratio (code elements without CoT justification)\n",
        "      correlates with test case failure rate.\n",
        "\n",
        "      High phantoms → Model generated code without reasoning it through\n",
        "                   → Higher probability of bugs\n",
        "\n",
        "  ┌─────────────────────────────────────────────────────────────────────────┐\n",
        "  │                                                                         │\n",
        "  │    phantom_ratio = |Phantom| / |CodeElements|                          │\n",
        "  │                                                                         │\n",
        "  │    where Phantom = { c ∈ Code | RD(c) = ∅ }                            │\n",
        "  │          RD(c) = reaching definitions from CoT segments                │\n",
        "  │                                                                         │\n",
        "  │    EXPECTED: Negative correlation with test pass rate                  │\n",
        "  │    PASS CRITERION: r < 0, p < 0.05                                     │\n",
        "  │                                                                         │\n",
        "  └─────────────────────────────────────────────────────────────────────────┘\n",
        "\n",
        "================================================================================\n",
        "                    WHY REACHING DEFINITIONS FOR COT?\n",
        "================================================================================\n",
        "\n",
        "  Classical Compiler Analysis:\n",
        "  ┌─────────────────────────────────────────────────────────────────────────┐\n",
        "  │                                                                         │\n",
        "  │    d1: x = 5          ──────┐                                          │\n",
        "  │    d2: y = x + 1            │ d1 reaches this use                      │\n",
        "  │    d3: x = 10         ──────┼──────┐                                   │\n",
        "  │    d4: z = x + y            │      │ d3 reaches, d1 killed             │\n",
        "  │                             ▼      ▼                                   │\n",
        "  │                                                                         │\n",
        "  │    \"For each USE of variable x, which DEFINITIONS could have           │\n",
        "  │     produced the value?\"                                               │\n",
        "  │                                                                         │\n",
        "  └─────────────────────────────────────────────────────────────────────────┘\n",
        "\n",
        "  CoT-DFA Mapping:\n",
        "  ┌─────────────────────────────┬─────────────────────────────────────────────┐\n",
        "  │ Program Analysis            │ CoT-DFA Equivalent                          │\n",
        "  ├─────────────────────────────┼─────────────────────────────────────────────┤\n",
        "  │ Variable definition         │ CoT step introducing concept/approach       │\n",
        "  │ Variable use                │ Code element using that concept             │\n",
        "  │ Reaching definition         │ Which CoT step justifies this code?         │\n",
        "  │ Dead code                   │ CoT steps not reaching any output           │\n",
        "  │ Use without definition      │ PHANTOM — code without reasoning            │\n",
        "  └─────────────────────────────┴─────────────────────────────────────────────┘\n",
        "\n",
        "================================================================================\n",
        "                    COT-DFA ANALYSIS EXAMPLE\n",
        "================================================================================\n",
        "\n",
        "  ┌─────────────────────────────────────────────────────────────────────────┐\n",
        "  │  CoT Trace:                                                            │\n",
        "  │  ┌───────────────────────────────────────────────────────────────┐    │\n",
        "  │  │ s1: \"First, I'll use a hash map for O(1) lookup\"              │    │\n",
        "  │  │ s2: \"I need to handle the edge case of empty input\"           │    │\n",
        "  │  │ s3: \"Let me add some comments for clarity\"                    │    │\n",
        "  │  └───────────────────────────────────────────────────────────────┘    │\n",
        "  │            │                    │                                      │\n",
        "  │            ▼                    ▼                                      │\n",
        "  │  Code Output:                                                          │\n",
        "  │  ┌───────────────────────────────────────────────────────────────┐    │\n",
        "  │  │ def solve(nums):                                               │    │\n",
        "  │  │     seen = {}  ◄─── s1 reaches (hash map → dict)              │    │\n",
        "  │  │     if not nums:  ◄─── s2 reaches (edge case → condition)     │    │\n",
        "  │  │         return -1                                              │    │\n",
        "  │  │     for n in nums:                                             │    │\n",
        "  │  │         seen[n] = True                                         │    │\n",
        "  │  │     return max(seen.keys()) ◄─── PHANTOM! (not in CoT)        │    │\n",
        "  │  └───────────────────────────────────────────────────────────────┘    │\n",
        "  │                                                                        │\n",
        "  │  Analysis Result:                                                      │\n",
        "  │  ├── s1 → LIVE (reaches hash map usage)                               │\n",
        "  │  ├── s2 → LIVE (reaches edge case check)                              │\n",
        "  │  ├── s3 → DEAD (no code element matches \"comments\")                   │\n",
        "  │  └── max(seen.keys()) → PHANTOM (not discussed in CoT)                │\n",
        "  │                                                                        │\n",
        "  │  Metrics:                                                              │\n",
        "  │  ├── phantom_ratio = 1/5 = 0.20 (one unjustified element)            │\n",
        "  │  ├── dead_ratio = 1/3 = 0.33 (one unproductive segment)              │\n",
        "  │  └── reach_coverage = 4/5 = 0.80 (80% code justified)                │\n",
        "  │                                                                        │\n",
        "  └─────────────────────────────────────────────────────────────────────────┘\n",
        "\n",
        "================================================================================\n",
        "                    COMPARISON: COT-DFA vs THOUGHT ANCHORS\n",
        "================================================================================\n",
        "\n",
        "  ┌─────────────────────────────────────────────────────────────────────────┐\n",
        "  │                                                                         │\n",
        "  │  THOUGHT ANCHORS (Bogdan et al.)     COT-DFA (This Work)               │\n",
        "  │  ──────────────────────────────      ─────────────────────              │\n",
        "  │                                                                         │\n",
        "  │  Question: \"Which sentences          Question: \"Is this a valid        │\n",
        "  │            matter causally?\"                    derivation?\"            │\n",
        "  │                                                                         │\n",
        "  │  Method:   Counterfactual            Method:   Structural analysis     │\n",
        "  │            perturbation                        (no model calls)        │\n",
        "  │                                                                         │\n",
        "  │  Cost:     O(n) forward passes       Cost:     O(1) - single pass      │\n",
        "  │            per sample                          parse + match           │\n",
        "  │                                                                         │\n",
        "  │  Detects:  • Important sentences     Detects:  • Phantom code          │\n",
        "  │            • Attention patterns                • Dead reasoning        │\n",
        "  │                                                                         │\n",
        "  │  ─────────────────────────────────────────────────────────────────     │\n",
        "  │                                                                         │\n",
        "  │  COMPLEMENTARY: Together they answer both                              │\n",
        "  │  \"What matters?\" AND \"Is it properly derived?\"                         │\n",
        "  │                                                                         │\n",
        "  └─────────────────────────────────────────────────────────────────────────┘\n",
        "\n",
        "================================================================================\n",
        "                    PRIOR WORK: UNFAITHFULNESS IS REAL\n",
        "================================================================================\n",
        "\n",
        "  ┌────────────────────────┬──────────────────────────┬────────────────────┐\n",
        "  │ Study                  │ Finding                  │ Implication        │\n",
        "  ├────────────────────────┼──────────────────────────┼────────────────────┤\n",
        "  │ Chen et al.            │ Claude 3.7 Sonnet only   │ Models frequently  │\n",
        "  │ (Anthropic, 2025)      │ 25% faithful on hint     │ don't say what     │\n",
        "  │                        │ verbalization test       │ they think         │\n",
        "  ├────────────────────────┼──────────────────────────┼────────────────────┤\n",
        "  │ Arcuschin et al.       │ GPT-4o-mini shows 13%    │ Unfaithfulness     │\n",
        "  │ (MATS, 2025)           │ implicit post-hoc        │ occurs naturally,  │\n",
        "  │ arXiv:2503.08679       │ rationalization rate     │ not just adversarial│\n",
        "  ├────────────────────────┼──────────────────────────┼────────────────────┤\n",
        "  │ Lanham et al.          │ Larger models produce    │ Problem may worsen │\n",
        "  │ (Anthropic, 2023)      │ less faithful reasoning  │ with scale         │\n",
        "  └────────────────────────┴──────────────────────────┴────────────────────┘\n",
        "\n",
        "  COT-DFA CONTRIBUTION: Lightweight, single-pass structural analysis that\n",
        "  complements expensive causal methods like Thought Anchors.\n",
        "\n",
        "================================================================================\n",
        "                    PIPELINE ARCHITECTURE\n",
        "================================================================================\n",
        "\n",
        "  ┌─────────────────────────────────────────────────────────────────────────┐\n",
        "  │                        COT-DFA PIPELINE                                 │\n",
        "  └─────────────────────────────────────────────────────────────────────────┘\n",
        "\n",
        "       ┌──────────┐      ┌──────────┐      ┌──────────┐      ┌──────────┐\n",
        "       │  INPUT   │      │  PARSE   │      │ ANALYZE  │      │  OUTPUT  │\n",
        "       │          │ ───► │          │ ───► │          │ ───► │          │\n",
        "       │ Model    │      │ CoT +    │      │ Reaching │      │ Metrics  │\n",
        "       │ Response │      │ Code     │      │ Defs     │      │ + Report │\n",
        "       └──────────┘      └──────────┘      └──────────┘      └──────────┘\n",
        "            │                 │                 │                 │\n",
        "            ▼                 ▼                 ▼                 ▼\n",
        "      ┌──────────┐      ┌──────────┐      ┌──────────┐      ┌──────────┐\n",
        "      │<think>   │      │Segments: │      │Def-Use   │      │phantom:  │\n",
        "      │...       │      │ s0, s1   │      │Graph     │      │ 0.15     │\n",
        "      │</think>  │      │          │      │          │      │dead:     │\n",
        "      │```python │      │Elements: │      │Reaching  │      │ 0.33     │\n",
        "      │def f():  │      │ c0, c1   │      │Sets      │      │faith:    │\n",
        "      │  ...     │      │          │      │          │      │ 0.72     │\n",
        "      └──────────┘      └──────────┘      └──────────┘      └──────────┘\n",
        "\n",
        "================================================================================\n",
        "                    DATA SOURCES\n",
        "================================================================================\n",
        "\n",
        "  ┌─────────────────────────────────────────────────────────────────────────┐\n",
        "  │ PRIMARY: OpenThoughts-114k                                             │\n",
        "  ├─────────────────────────────────────────────────────────────────────────┤\n",
        "  │ Source:  HuggingFace (open-thoughts/OpenThoughts-114k)                 │\n",
        "  │ Content: 114K reasoning traces from DeepSeek-R1                        │\n",
        "  │ Format:  problem, deepseek_reasoning (<think>), deepseek_solution      │\n",
        "  │ Filter:  domain == \"code\" (TACO, APPS, CodeContests)                   │\n",
        "  │ Sample:  100 problems with test cases                                  │\n",
        "  │ Advantage: Pre-existing high-quality CoT traces                        │\n",
        "  └─────────────────────────────────────────────────────────────────────────┘\n",
        "\n",
        "  ┌─────────────────────────────────────────────────────────────────────────┐\n",
        "  │ SECONDARY: HumanEval (Fresh Generations)                               │\n",
        "  ├─────────────────────────────────────────────────────────────────────────┤\n",
        "  │ Source:  HuggingFace (openai_humaneval)                                │\n",
        "  │ Content: 164 Python programming problems                               │\n",
        "  │ Model:   CodeGemma 7B-IT (prompted for <think> blocks)                 │\n",
        "  │ Sample:  50 problems                                                   │\n",
        "  │ Advantage: Validate on model we control                                │\n",
        "  └─────────────────────────────────────────────────────────────────────────┘\n",
        "\n",
        "  ┌─────────────────────────────────────────────────────────────────────────┐\n",
        "  │ REFERENCE: chainscope                                                  │\n",
        "  ├─────────────────────────────────────────────────────────────────────────┤\n",
        "  │ Source:  GitHub (jettjaniak/chainscope)                                │\n",
        "  │ Content: Labeled unfaithful CoT examples from Arcuschin et al.         │\n",
        "  │ Patterns: post_hoc, restoration, shortcut                              │\n",
        "  │ Purpose: Calibrate unfaithfulness detection                            │\n",
        "  └─────────────────────────────────────────────────────────────────────────┘\n",
        "\n",
        "================================================================================\n",
        "                    TECHNICAL STACK\n",
        "================================================================================\n",
        "\n",
        "  ┌─────────────────┬─────────────────────────┬─────────────────────────────┐\n",
        "  │ Component       │ Choice                  │ Rationale                   │\n",
        "  ├─────────────────┼─────────────────────────┼─────────────────────────────┤\n",
        "  │ Platform        │ Google Colab Pro (H100) │ 80GB VRAM, JAX native       │\n",
        "  │ Model           │ CodeGemma 7B-IT         │ JAX/Flax, MLIR-compatible   │\n",
        "  │ Model Load      │ kagglehub               │ Official Google pathway     │\n",
        "  │ Embeddings      │ UniXcoder               │ Code-NL shared space        │\n",
        "  │ AST Analysis    │ beniget + ast           │ Lightweight def-use chains  │\n",
        "  │ Framework       │ JAX/Flax                │ XLA compilation, TPU-ready  │\n",
        "  │ Statistics      │ scipy                   │ Point-biserial, Fisher's    │\n",
        "  │ Visualization   │ matplotlib + seaborn    │ Publication-quality plots   │\n",
        "  └─────────────────┴─────────────────────────┴─────────────────────────────┘\n",
        "\n",
        "================================================================================\n",
        "                    CONCEPT VOCABULARY (22 PROGRAMMING CONCEPTS)\n",
        "================================================================================\n",
        "\n",
        "  ┌─────────────────────────────────────────────────────────────────────────┐\n",
        "  │ DATA STRUCTURES                                                        │\n",
        "  ├─────────────────────────────────────────────────────────────────────────┤\n",
        "  │ dict  │ hash, map, dictionary, hashmap, key-value, lookup, counter     │\n",
        "  │ list  │ array, list, sequence, collection, elements, items             │\n",
        "  │ set   │ set, unique, deduplicate, distinct                             │\n",
        "  │ stack │ stack, lifo, push, pop                                         │\n",
        "  │ queue │ queue, fifo, deque, bfs                                        │\n",
        "  │ heap  │ heap, priority queue, heapq, min heap, max heap                │\n",
        "  │ tree  │ tree, binary tree, bst, trie, node, root                       │\n",
        "  │ graph │ graph, vertices, edges, adjacent, neighbor, dfs                │\n",
        "  └─────────────────────────────────────────────────────────────────────────┘\n",
        "\n",
        "  ┌─────────────────────────────────────────────────────────────────────────┐\n",
        "  │ ALGORITHMS                                                             │\n",
        "  ├─────────────────────────────────────────────────────────────────────────┤\n",
        "  │ sort       │ sort, order, arrange, sorted, ascending, descending       │\n",
        "  │ search     │ search, find, lookup, binary search, locate               │\n",
        "  │ recursion  │ recursive, recursion, base case, call itself              │\n",
        "  │ dp         │ dynamic programming, memoization, memo, dp, subproblem    │\n",
        "  │ greedy     │ greedy, local optimal, best choice                        │\n",
        "  │ two_pointer│ two pointer, left right, start end, sliding window        │\n",
        "  │ backtrack  │ backtrack, prune, explore, candidates                     │\n",
        "  └─────────────────────────────────────────────────────────────────────────┘\n",
        "\n",
        "  ┌─────────────────────────────────────────────────────────────────────────┐\n",
        "  │ CONTROL FLOW                                                           │\n",
        "  ├─────────────────────────────────────────────────────────────────────────┤\n",
        "  │ loop        │ iterate, loop, for each, traverse, go through, while     │\n",
        "  │ condition   │ if, check, condition, edge case, boundary                │\n",
        "  │ early_return│ return early, base case, edge case, special case         │\n",
        "  └─────────────────────────────────────────────────────────────────────────┘\n",
        "\n",
        "  ┌─────────────────────────────────────────────────────────────────────────┐\n",
        "  │ OPERATIONS                                                             │\n",
        "  ├─────────────────────────────────────────────────────────────────────────┤\n",
        "  │ count     │ count, frequency, occurrences, how many                    │\n",
        "  │ sum       │ sum, total, add up, accumulate                             │\n",
        "  │ max_min   │ maximum, minimum, max, min, largest, smallest              │\n",
        "  │ string_op │ string, character, substring, split, join                  │\n",
        "  └─────────────────────────────────────────────────────────────────────────┘\n",
        "\n",
        "================================================================================\n",
        "                    METRICS DEFINITIONS\n",
        "================================================================================\n",
        "\n",
        "  ┌─────────────────────────────────────────────────────────────────────────┐\n",
        "  │ PHANTOM RATIO                                                          │\n",
        "  ├─────────────────────────────────────────────────────────────────────────┤\n",
        "  │                                                                         │\n",
        "  │                  |Phantom|        # code elements without CoT          │\n",
        "  │ phantom_ratio = ─────────── = ──────────────────────────────────       │\n",
        "  │                    |C|              # total code elements              │\n",
        "  │                                                                         │\n",
        "  │ Interpretation:                                                        │\n",
        "  │ • 0.0 = Perfect: Every code element has CoT justification              │\n",
        "  │ • 0.5 = Concerning: Half the code \"appeared from nowhere\"              │\n",
        "  │ • 1.0 = Complete disconnect: CoT irrelevant to code                    │\n",
        "  │                                                                         │\n",
        "  │ HYPOTHESIS: High phantom_ratio → test failures                         │\n",
        "  │                                                                         │\n",
        "  └─────────────────────────────────────────────────────────────────────────┘\n",
        "\n",
        "  ┌─────────────────────────────────────────────────────────────────────────┐\n",
        "  │ DEAD RATIO                                                             │\n",
        "  ├─────────────────────────────────────────────────────────────────────────┤\n",
        "  │                                                                         │\n",
        "  │                |Dead|          # CoT steps reaching nothing            │\n",
        "  │ dead_ratio = ────────── = ─────────────────────────────────────        │\n",
        "  │                |S|               # total CoT segments                  │\n",
        "  │                                                                         │\n",
        "  │ Interpretation:                                                        │\n",
        "  │ • 0.0 = Efficient: Every reasoning step contributes                    │\n",
        "  │ • 0.3 = Normal: Some exploratory thinking                              │\n",
        "  │ • 0.7+ = Suspicious: Mostly filler/padding                             │\n",
        "  │                                                                         │\n",
        "  └─────────────────────────────────────────────────────────────────────────┘\n",
        "\n",
        "  ┌─────────────────────────────────────────────────────────────────────────┐\n",
        "  │ FAITHFULNESS SCORE (Combined)                                          │\n",
        "  ├─────────────────────────────────────────────────────────────────────────┤\n",
        "  │                                                                         │\n",
        "  │ faithfulness = α × structural_score + β × semantic_similarity          │\n",
        "  │                                                                         │\n",
        "  │ where:                                                                  │\n",
        "  │   structural_score = reach_coverage × (1 - 0.5 × dead_ratio)           │\n",
        "  │   reach_coverage = 1 - phantom_ratio                                   │\n",
        "  │   α = 0.7, β = 0.3 (tunable weights)                                   │\n",
        "  │                                                                         │\n",
        "  │ Interpretation:                                                        │\n",
        "  │ • 0.0-0.3: Low faithfulness (CoT disconnected from code)               │\n",
        "  │ • 0.3-0.6: Moderate faithfulness (partial alignment)                   │\n",
        "  │ • 0.6-1.0: High faithfulness (CoT reflects code structure)             │\n",
        "  │                                                                         │\n",
        "  └─────────────────────────────────────────────────────────────────────────┘\n",
        "\n",
        "  ┌─────────────────────────────────────────────────────────────────────────┐\n",
        "  │ CONCEPT JACCARD                                                        │\n",
        "  ├─────────────────────────────────────────────────────────────────────────┤\n",
        "  │                                                                         │\n",
        "  │                |κ(S) ∩ κ(C)|      # shared concepts                    │\n",
        "  │ jaccard = ───────────────────── = ─────────────────────────────        │\n",
        "  │            |κ(S) ∪ κ(C)|           # total unique concepts             │\n",
        "  │                                                                         │\n",
        "  │ where κ(S) = concepts from CoT segments, κ(C) = concepts from code     │\n",
        "  │                                                                         │\n",
        "  └─────────────────────────────────────────────────────────────────────────┘\n",
        "\n",
        "================================================================================\n",
        "                    STATISTICAL ANALYSIS PLAN\n",
        "================================================================================\n",
        "\n",
        "  For small samples (n=50-150), we use robust non-parametric methods:\n",
        "\n",
        "  ┌─────────────────────────────────────────────────────────────────────────┐\n",
        "  │ 1. POINT-BISERIAL CORRELATION                                          │\n",
        "  ├─────────────────────────────────────────────────────────────────────────┤\n",
        "  │ Use: Continuous (faithfulness) vs Binary (pass/fail)                   │\n",
        "  │ Power: Adequate at n=50 for medium-large effects (r ≥ 0.3)             │\n",
        "  │ Implementation: scipy.stats.pointbiserialr()                           │\n",
        "  │ Expected: r < 0 for phantom_ratio (negative correlation)               │\n",
        "  └─────────────────────────────────────────────────────────────────────────┘\n",
        "\n",
        "  ┌─────────────────────────────────────────────────────────────────────────┐\n",
        "  │ 2. FISHER'S EXACT TEST                                                 │\n",
        "  ├─────────────────────────────────────────────────────────────────────────┤\n",
        "  │ Use: 2×2 contingency (faithful/unfaithful × correct/incorrect)         │\n",
        "  │ Advantage: No minimum sample requirement                               │\n",
        "  │ Report: Odds ratio with 95% CI                                         │\n",
        "  │ Implementation: scipy.stats.fisher_exact()                             │\n",
        "  └─────────────────────────────────────────────────────────────────────────┘\n",
        "\n",
        "  ┌─────────────────────────────────────────────────────────────────────────┐\n",
        "  │ 3. BOOTSTRAP CONFIDENCE INTERVALS                                      │\n",
        "  ├─────────────────────────────────────────────────────────────────────────┤\n",
        "  │ Method: BCa (bias-corrected and accelerated)                           │\n",
        "  │ Resamples: 9,999                                                       │\n",
        "  │ Use: Robust uncertainty quantification for effect sizes                │\n",
        "  └─────────────────────────────────────────────────────────────────────────┘\n",
        "\n",
        "  ┌─────────────────────────────────────────────────────────────────────────┐\n",
        "  │ 4. EFFECT SIZES (Cohen's d)                                            │\n",
        "  ├─────────────────────────────────────────────────────────────────────────┤\n",
        "  │ |d| < 0.2:  Negligible                                                 │\n",
        "  │ 0.2 ≤ |d| < 0.5:  Small                                                │\n",
        "  │ 0.5 ≤ |d| < 0.8:  Medium                                               │\n",
        "  │ |d| ≥ 0.8:  Large (TARGET for PoC)                                     │\n",
        "  │                                                                         │\n",
        "  │ Always report effect sizes alongside p-values                          │\n",
        "  └─────────────────────────────────────────────────────────────────────────┘\n",
        "\n",
        "  SCIENTIFIC NOTE: Even null results are publishable if methodology is\n",
        "  sound and framework is mechanically validated.\n",
        "\n",
        "================================================================================\n",
        "                    20-STEP EXECUTION PLAN (15 HOURS)\n",
        "================================================================================\n",
        "\n",
        "┌──────┬─────────────────────────────────────────────┬────────┬──────────────┐\n",
        "│ Step │ Description                                 │ Time   │ Deliverable  │\n",
        "├──────┼─────────────────────────────────────────────┼────────┼──────────────┤\n",
        "│      │ PHASE 1: ENVIRONMENT & DATA SETUP (1.5h)   │        │              │\n",
        "├──────┼─────────────────────────────────────────────┼────────┼──────────────┤\n",
        "│  1   │ Verify GPU, install dependencies           │ 20 min │ Environment  │\n",
        "│  2   │ Load CodeGemma 7B-IT via kagglehub         │ 25 min │ Sampler obj  │\n",
        "│  3   │ Download OpenThoughts-114k, filter code    │ 15 min │ 100 samples  │\n",
        "│  4   │ Clone chainscope, define unfaithful patterns│ 20 min │ Reference    │\n",
        "├──────┼─────────────────────────────────────────────┼────────┼──────────────┤\n",
        "│      │ PHASE 2: EXTRACTION PIPELINE (2.5h)        │        │              │\n",
        "├──────┼─────────────────────────────────────────────┼────────┼──────────────┤\n",
        "│  5   │ Build CoT sentence segmenter               │ 30 min │ segment_cot()│\n",
        "│  6   │ Build concept vocabulary + CoT extractor   │ 45 min │ extract_cot_ │\n",
        "│  7   │ Build AST concept extractor with beniget   │ 45 min │ extract_code_│\n",
        "│  8   │ Build reaching definitions analyzer        │ 30 min │ compute_rd() │\n",
        "├──────┼─────────────────────────────────────────────┼────────┼──────────────┤\n",
        "│      │ PHASE 3: METRICS DEVELOPMENT (2.5h)        │        │              │\n",
        "├──────┼─────────────────────────────────────────────┼────────┼──────────────┤\n",
        "│  9   │ Implement phantom_ratio, dead_ratio        │ 45 min │ Metric funcs │\n",
        "│ 10   │ Implement reach_coverage, concept_jaccard  │ 45 min │ Metric funcs │\n",
        "│ 11   │ Implement UniXcoder semantic similarity    │ 30 min │ semantic_sim │\n",
        "│ 12   │ Combine into faithfulness_score            │ 30 min │ compute_f()  │\n",
        "├──────┼─────────────────────────────────────────────┼────────┼──────────────┤\n",
        "│      │ PHASE 4: EVALUATION FRAMEWORK (3h)         │        │              │\n",
        "├──────┼─────────────────────────────────────────────┼────────┼──────────────┤\n",
        "│ 13   │ Build safe code execution harness          │ 45 min │ execute_safe │\n",
        "│ 14   │ Run analysis on 100 OpenThoughts samples   │ 60 min │ Results list │\n",
        "│ 15   │ Load HumanEval, generate 50 CodeGemma CoTs │ 45 min │ Fresh gens   │\n",
        "│ 16   │ Combine into final dataset (150 samples)   │ 30 min │ combined_df  │\n",
        "├──────┼─────────────────────────────────────────────┼────────┼──────────────┤\n",
        "│      │ PHASE 5: STATISTICAL ANALYSIS (2.5h)       │        │              │\n",
        "├──────┼─────────────────────────────────────────────┼────────┼──────────────┤\n",
        "│ 17   │ Point-biserial correlation                 │ 30 min │ r, p-value   │\n",
        "│ 18   │ Fisher's exact test + odds ratio           │ 30 min │ OR, 95% CI   │\n",
        "│ 19   │ Bootstrap CIs + Cohen's d effect sizes     │ 30 min │ Effect sizes │\n",
        "│ 20   │ Generate visualizations + final report     │ 60 min │ 4 figs+report│\n",
        "└──────┴─────────────────────────────────────────────┴────────┴──────────────┘\n",
        "\n",
        "  TOTAL: 15 hours execution + ~3 hours buffer for debugging\n",
        "\n",
        "================================================================================\n",
        "                    VALIDATION TESTS PER CELL\n",
        "================================================================================\n",
        "\n",
        "  CELL 1 (Setup - Step 1):\n",
        "  ├── nvidia-smi shows GPU available\n",
        "  ├── All pip installs succeed\n",
        "  ├── import torch succeeds\n",
        "  ├── import jax succeeds (optional)\n",
        "  ├── import transformers succeeds\n",
        "  └── CUDA/CPU device detected correctly\n",
        "\n",
        "  CELL 2 (Model - Step 2):\n",
        "  ├── kagglehub.model_download succeeds\n",
        "  ├── tokenizer.Load() succeeds\n",
        "  ├── params_lib.load_and_format_params() succeeds\n",
        "  ├── sampler generates text\n",
        "  └── Test generation produces valid Python-like output\n",
        "\n",
        "  CELL 3 (Dataset - Step 3):\n",
        "  ├── load_dataset(\"open-thoughts/OpenThoughts-114k\") succeeds\n",
        "  ├── DataFrame has expected columns\n",
        "  ├── Filter returns >100 code samples\n",
        "  ├── Sample of 100 created successfully\n",
        "  └── Test cases present in samples\n",
        "\n",
        "  CELL 4 (Reference - Step 4):\n",
        "  ├── git clone chainscope succeeds (or graceful skip)\n",
        "  ├── UNFAITHFUL_PATTERNS dict populated\n",
        "  └── Reference patterns accessible\n",
        "\n",
        "  CELL 5 (Segmenter - Step 5):\n",
        "  ├── segment_cot() returns List[Segment]\n",
        "  ├── Segment has id, text, position, concepts\n",
        "  ├── <think> block extraction works\n",
        "  ├── Sentence splitting works\n",
        "  ├── Numbered step splitting works\n",
        "  └── Minimum length filter applied (>15 chars)\n",
        "\n",
        "  CELL 6 (CoT Concepts - Step 6):\n",
        "  ├── CONCEPT_VOCABULARY has 22 concepts\n",
        "  ├── extract_cot_concepts() returns Set[str]\n",
        "  ├── \"hash map\" → 'dict' mapping works\n",
        "  ├── Multiple concepts extracted from single segment\n",
        "  └── Empty segment returns empty set\n",
        "\n",
        "  CELL 7 (AST Concepts - Step 7):\n",
        "  ├── extract_code_concepts() returns (Set, List[CodeElement])\n",
        "  ├── ast.Dict → 'dict' mapping works\n",
        "  ├── ast.For → 'loop' mapping works\n",
        "  ├── Function call detection (sorted→sort) works\n",
        "  ├── CodeElement has id, node_type, line_number, concepts\n",
        "  └── SyntaxError gracefully returns empty\n",
        "\n",
        "  CELL 8 (Reaching Defs - Step 8):\n",
        "  ├── compute_reaching_definitions() returns DFAResult\n",
        "  ├── DFAResult has segments, elements, reaching_sets\n",
        "  ├── reaching_sets[elem.id] is ReachingSet\n",
        "  ├── Concept overlap creates edges\n",
        "  ├── phantoms property returns elements with no reaching\n",
        "  └── dead_segments property returns segments reaching nothing\n",
        "\n",
        "  CELL 9-10 (Ratios - Steps 9-10):\n",
        "  ├── phantom_ratio() returns float in [0, 1]\n",
        "  ├── dead_ratio() returns float in [0, 1]\n",
        "  ├── reach_coverage = 1 - phantom_ratio (verified)\n",
        "  ├── concept_jaccard() returns float in [0, 1]\n",
        "  └── Edge cases (empty) return 0.0\n",
        "\n",
        "  CELL 11 (UniXcoder - Step 11):\n",
        "  ├── AutoTokenizer.from_pretrained succeeds\n",
        "  ├── AutoModel.from_pretrained succeeds\n",
        "  ├── get_embedding() returns 768-dim vector\n",
        "  ├── semantic_similarity() returns float in [-1, 1]\n",
        "  └── \"hash map\" similar to \"{}\" (positive cosine)\n",
        "\n",
        "  CELL 12 (Faithfulness - Step 12):\n",
        "  ├── compute_faithfulness() returns FaithfulnessResult\n",
        "  ├── FaithfulnessResult has all component metrics\n",
        "  ├── faithfulness_score in [0, 1]\n",
        "  ├── Weights α=0.7, β=0.3 applied correctly\n",
        "  └── All fields populated\n",
        "\n",
        "  CELL 13 (Execution - Step 13):\n",
        "  ├── execute_code_safely() returns (bool, str, str)\n",
        "  ├── Timeout works (10s default)\n",
        "  ├── Passing code returns (True, stdout, \"\")\n",
        "  ├── Failing code returns (False, \"\", stderr)\n",
        "  └── Temp file cleaned up\n",
        "\n",
        "  CELL 14 (OpenThoughts - Step 14):\n",
        "  ├── analyze_sample() returns AnalysisResult\n",
        "  ├── All 100 samples processed\n",
        "  ├── Results list has 100 entries\n",
        "  ├── Progress bar (tqdm) works\n",
        "  └── No crashes on edge cases\n",
        "\n",
        "  CELL 15 (HumanEval - Step 15):\n",
        "  ├── load_dataset(\"openai_humaneval\") succeeds\n",
        "  ├── 50 problems selected\n",
        "  ├── CodeGemma generation works\n",
        "  ├── <think> blocks parsed\n",
        "  └── ```python``` blocks parsed\n",
        "\n",
        "  CELL 16 (Combine - Step 16):\n",
        "  ├── results_df has 150 rows\n",
        "  ├── All columns present\n",
        "  ├── source column distinguishes OT vs HE\n",
        "  ├── describe() shows reasonable stats\n",
        "  └── No NaN in critical columns\n",
        "\n",
        "  CELL 17 (Correlation - Step 17):\n",
        "  ├── pointbiserialr() returns (r, p)\n",
        "  ├── r is in [-1, 1]\n",
        "  ├── p is in [0, 1]\n",
        "  ├── All metrics tested\n",
        "  └── correlation_results dict populated\n",
        "\n",
        "  CELL 18 (Fisher - Step 18):\n",
        "  ├── Contingency table created\n",
        "  ├── fisher_exact() returns (OR, p)\n",
        "  ├── Odds ratio is positive\n",
        "  ├── 95% CI computed via log transform\n",
        "  └── fisher_results dict populated\n",
        "\n",
        "  CELL 19 (Effect Sizes - Step 19):\n",
        "  ├── cohens_d() returns float\n",
        "  ├── Bootstrap resampling works (9999)\n",
        "  ├── 95% CI computed\n",
        "  ├── Interpretation string correct\n",
        "  └── effect_results dict populated\n",
        "\n",
        "  CELL 20 (Report - Step 20):\n",
        "  ├── 4 visualizations created\n",
        "  ├── Figures saved to disk\n",
        "  ├── Report markdown generated\n",
        "  ├── All placeholders filled\n",
        "  └── Files downloadable\n",
        "\n",
        "================================================================================\n",
        "                    MEMORY BUDGET (Google Colab H100)\n",
        "================================================================================\n",
        "\n",
        "  ┌────────────────────────────────┬─────────────┬──────────────────────────┐\n",
        "  │ Component                      │ Memory      │ Notes                    │\n",
        "  ├────────────────────────────────┼─────────────┼──────────────────────────┤\n",
        "  │ CodeGemma 7B-IT weights        │ ~14 GB      │ bf16: 7B × 2 bytes       │\n",
        "  │ CodeGemma activations          │ ~2 GB       │ Inference batch          │\n",
        "  │ UniXcoder model                │ ~500 MB     │ 125M params              │\n",
        "  │ OpenThoughts samples           │ ~100 MB     │ 100 samples in memory    │\n",
        "  │ Results DataFrame              │ ~50 MB      │ 150 rows, all columns    │\n",
        "  │ Working memory                 │ ~2 GB       │ Intermediate tensors     │\n",
        "  ├────────────────────────────────┼─────────────┼──────────────────────────┤\n",
        "  │ TOTAL                          │ ~19 GB      │ Fits H100 80GB easily    │\n",
        "  └────────────────────────────────┴─────────────┴──────────────────────────┘\n",
        "\n",
        "  FALLBACK: If CodeGemma loading fails, use only OpenThoughts (no Step 15)\n",
        "            This reduces to ~4GB total, runnable on T4.\n",
        "\n",
        "================================================================================\n",
        "                    OUTPUT DATA STRUCTURES\n",
        "================================================================================\n",
        "\n",
        "  @dataclass\n",
        "  class Segment:\n",
        "      id: str                    # \"s0\", \"s1\", ...\n",
        "      text: str                  # Raw CoT text\n",
        "      position: int              # Order in CoT\n",
        "      concepts: Set[str]         # Extracted concepts\n",
        "\n",
        "  @dataclass\n",
        "  class CodeElement:\n",
        "      id: str                    # \"c0\", \"c1\", ...\n",
        "      node_type: str             # \"Dict\", \"For\", \"Call\", ...\n",
        "      line_number: int           # Source location\n",
        "      concepts: Set[str]         # Extracted concepts\n",
        "\n",
        "  @dataclass\n",
        "  class ReachingSet:\n",
        "      element: CodeElement\n",
        "      reaching_segments: Set[str]  # Segment IDs that reach\n",
        "\n",
        "  @dataclass\n",
        "  class DFAResult:\n",
        "      segments: List[Segment]\n",
        "      elements: List[CodeElement]\n",
        "      reaching_sets: Dict[str, ReachingSet]\n",
        "      cot_concepts: Set[str]\n",
        "      code_concepts: Set[str]\n",
        "\n",
        "  @dataclass\n",
        "  class FaithfulnessResult:\n",
        "      phantom_ratio: float\n",
        "      dead_ratio: float\n",
        "      reach_coverage: float\n",
        "      semantic_sim: float\n",
        "      faithfulness_score: float\n",
        "      cot_concepts: Set[str]\n",
        "      code_concepts: Set[str]\n",
        "      concept_overlap: Set[str]\n",
        "      concept_jaccard: float\n",
        "      num_segments: int\n",
        "      num_elements: int\n",
        "      num_phantoms: int\n",
        "      num_dead: int\n",
        "\n",
        "  @dataclass\n",
        "  class AnalysisResult:\n",
        "      sample_id: str\n",
        "      faithfulness: FaithfulnessResult\n",
        "      test_passed: bool\n",
        "      execution_error: Optional[str]\n",
        "\n",
        "================================================================================\n",
        "                    DEPENDENCIES\n",
        "================================================================================\n",
        "\n",
        "  Core:\n",
        "  ├── kagglehub              # CodeGemma download\n",
        "  ├── gemma                  # CodeGemma inference\n",
        "  ├── flax                   # JAX neural networks\n",
        "  ├── jax[cuda12]            # XLA compilation\n",
        "  ├── transformers           # UniXcoder, datasets\n",
        "  ├── datasets               # HuggingFace datasets\n",
        "  ├── sentencepiece          # Tokenization\n",
        "  └── torch                  # UniXcoder backend\n",
        "\n",
        "  Analysis:\n",
        "  ├── scipy                  # Statistical tests\n",
        "  ├── pandas                 # DataFrames\n",
        "  ├── numpy                  # Numerics\n",
        "  └── beniget                # AST def-use chains\n",
        "\n",
        "  Visualization:\n",
        "  ├── matplotlib             # Plots\n",
        "  ├── seaborn                # Statistical visualization\n",
        "  └── tqdm                   # Progress bars\n",
        "\n",
        "================================================================================\n",
        "                    ALIGNMENT WITH NEEL NANDA'S INTERESTS\n",
        "================================================================================\n",
        "\n",
        "  ┌──────────────────────────────┬──────────────────────────────┬──────────┐\n",
        "  │ Neel's Interest              │ CoT-DFA Addresses            │ Match    │\n",
        "  ├──────────────────────────────┼──────────────────────────────┼──────────┤\n",
        "  │ \"Can we tell when CoT was    │ Reaching definitions track   │          │\n",
        "  │  causally important for      │ exactly which CoT segments   │    ✓     │\n",
        "  │  giving its answer?\"         │ contribute to code elements  │          │\n",
        "  ├──────────────────────────────┼──────────────────────────────┼──────────┤\n",
        "  │ \"Design good monitors or     │ phantom_ratio, dead_ratio,   │          │\n",
        "  │  metrics for whether CoT     │ faithfulness_score are       │    ✓     │\n",
        "  │  is telling us what we       │ exactly this type of metric  │          │\n",
        "  │  think?\"                     │                              │          │\n",
        "  ├──────────────────────────────┼──────────────────────────────┼──────────┤\n",
        "  │ \"Extend Thought Anchors\"     │ Complementary formalism:     │          │\n",
        "  │                              │ • Thought Anchors: causal    │    ✓     │\n",
        "  │                              │ • CoT-DFA: structural        │          │\n",
        "  ├──────────────────────────────┼──────────────────────────────┼──────────┤\n",
        "  │ \"Reasoning models\"           │ Targets code generation      │          │\n",
        "  │                              │ with native <think> blocks   │    ✓     │\n",
        "  ├──────────────────────────────┼──────────────────────────────┼──────────┤\n",
        "  │ \"Applied interpretability\"   │ Single-pass, no expensive    │          │\n",
        "  │                              │ resampling, production-ready │    ✓     │\n",
        "  ├──────────────────────────────┼──────────────────────────────┼──────────┤\n",
        "  │ \"Start simple\"               │ Classical compiler analysis  │          │\n",
        "  │                              │ applied to new domain        │    ✓     │\n",
        "  └──────────────────────────────┴──────────────────────────────┴──────────┘\n",
        "\n",
        "================================================================================\n",
        "                    EXTENSION: CIRCUIT PROVENANCE BRIDGE\n",
        "================================================================================\n",
        "\n",
        "  If CoT-DFA validates, bridge to training data attribution:\n",
        "\n",
        "  ┌─────────────────────────────────────────────────────────────────────────┐\n",
        "  │                                                                         │\n",
        "  │  H₅: Training examples with shortcuts (correct answer, weak reasoning) │\n",
        "  │      cause models to produce unfaithful Chain-of-Thought.              │\n",
        "  │                                                                         │\n",
        "  │  ┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐   │\n",
        "  │  │  CoT-DFA        │     │  Influence      │     │  Compare        │   │\n",
        "  │  │  Classify:      │ ──► │  Functions:     │ ──► │  Training       │   │\n",
        "  │  │  faithful vs    │     │  Find top-K     │     │  Examples       │   │\n",
        "  │  │  unfaithful     │     │  influencers    │     │                 │   │\n",
        "  │  └─────────────────┘     └─────────────────┘     └─────────────────┘   │\n",
        "  │                                                                         │\n",
        "  │  PASS: shortcut_prevalence(unfaithful) > shortcut_prevalence(faithful) │\n",
        "  │                                                                         │\n",
        "  └─────────────────────────────────────────────────────────────────────────┘\n",
        "\n",
        "  This connects to the broader IAM-Audit dissertation work on compiler-\n",
        "  integrated interpretability.\n",
        "\n",
        "================================================================================\n",
        "                    WHAT THIS NOTEBOOK PROVES\n",
        "================================================================================\n",
        "\n",
        "  ✓ Reaching definitions can be applied to CoT → code analysis\n",
        "  ✓ phantom_ratio correlates (or not) with test failure\n",
        "  ✓ Lightweight structural analysis complements causal methods\n",
        "  ✓ 22 programming concepts suffice for code generation domain\n",
        "  ✓ UniXcoder provides semantic validation of structural matching\n",
        "  ✓ Statistical framework appropriate for small samples\n",
        "  ✓ Pipeline scales to production deployment\n",
        "\n",
        "================================================================================\n",
        "                    SUCCESS CRITERIA\n",
        "================================================================================\n",
        "\n",
        "  PRIMARY (H₁):\n",
        "  ├── Significant negative correlation (p < 0.05)\n",
        "  ├── phantom_ratio vs test pass: r < 0\n",
        "  └── Effect size: Cohen's d ≥ 0.5 (medium or larger)\n",
        "\n",
        "  SECONDARY:\n",
        "  ├── dead_ratio > 0.3 indicates padding behavior\n",
        "  ├── Harder problems → higher phantom_ratio\n",
        "  └── Phantom locations correlate with bug locations\n",
        "\n",
        "  INTERESTING FAILURE:\n",
        "  ├── Framework mechanically validated even if H₁ fails\n",
        "  ├── Null result still publishable with proper analysis\n",
        "  └── Opens questions for future research\n",
        "\n",
        "================================================================================\n",
        "\"\"\"\n",
        "\n",
        "print(__doc__)\n",
        "\n",
        "# ============================================================================\n",
        "# Configuration Registry\n",
        "# ============================================================================\n",
        "\n",
        "CONFIG = {\n",
        "    \"model\": {\n",
        "        \"name\": \"google/codegemma/flax/7b-it\",\n",
        "        \"provider\": \"kagglehub\",\n",
        "        \"params\": \"7B\",\n",
        "        \"memory_gb\": 14,\n",
        "    },\n",
        "    \"embeddings\": {\n",
        "        \"name\": \"microsoft/unixcoder-base\",\n",
        "        \"dim\": 768,\n",
        "        \"memory_mb\": 500,\n",
        "    },\n",
        "    \"dataset\": {\n",
        "        \"primary\": \"open-thoughts/OpenThoughts-114k\",\n",
        "        \"secondary\": \"openai_humaneval\",\n",
        "        \"reference\": \"jettjaniak/chainscope\",\n",
        "        \"n_primary\": 100,\n",
        "        \"n_secondary\": 50,\n",
        "        \"n_total\": 150,\n",
        "    },\n",
        "    \"concepts\": {\n",
        "        \"n_concepts\": 22,\n",
        "        \"categories\": [\"data_structures\", \"algorithms\", \"control_flow\", \"operations\"],\n",
        "    },\n",
        "    \"metrics\": {\n",
        "        \"alpha\": 0.7,  # Structural weight\n",
        "        \"beta\": 0.3,   # Semantic weight\n",
        "    },\n",
        "    \"statistics\": {\n",
        "        \"significance_level\": 0.05,\n",
        "        \"bootstrap_resamples\": 9999,\n",
        "        \"effect_size_target\": 0.5,  # Medium\n",
        "    },\n",
        "    \"execution\": {\n",
        "        \"timeout_seconds\": 10,\n",
        "        \"random_seed\": 42,\n",
        "    },\n",
        "}\n",
        "\n",
        "# ============================================================================\n",
        "# Concept Vocabulary\n",
        "# ============================================================================\n",
        "\n",
        "CONCEPT_VOCABULARY = {\n",
        "    # Data Structures (8)\n",
        "    'dict': {'hash', 'map', 'dictionary', 'hashmap', 'hash map', 'key-value',\n",
        "             'lookup table', 'mapping', 'counter'},\n",
        "    'list': {'array', 'list', 'sequence', 'collection', 'elements', 'items'},\n",
        "    'set': {'set', 'unique', 'deduplicate', 'distinct'},\n",
        "    'stack': {'stack', 'lifo', 'push', 'pop'},\n",
        "    'queue': {'queue', 'fifo', 'deque', 'bfs'},\n",
        "    'heap': {'heap', 'priority queue', 'heapq', 'min heap', 'max heap'},\n",
        "    'tree': {'tree', 'binary tree', 'bst', 'trie', 'node', 'root'},\n",
        "    'graph': {'graph', 'vertices', 'edges', 'adjacent', 'neighbor', 'dfs'},\n",
        "\n",
        "    # Algorithms (7)\n",
        "    'sort': {'sort', 'order', 'arrange', 'sorted', 'ascending', 'descending'},\n",
        "    'search': {'search', 'find', 'lookup', 'binary search', 'locate'},\n",
        "    'recursion': {'recursive', 'recursion', 'base case', 'call itself'},\n",
        "    'dp': {'dynamic programming', 'memoization', 'memo', 'dp', 'subproblem'},\n",
        "    'greedy': {'greedy', 'local optimal', 'best choice'},\n",
        "    'two_pointer': {'two pointer', 'left right', 'start end', 'sliding window'},\n",
        "    'backtrack': {'backtrack', 'prune', 'explore', 'candidates'},\n",
        "\n",
        "    # Control Flow (3)\n",
        "    'loop': {'iterate', 'loop', 'for each', 'traverse', 'go through', 'while'},\n",
        "    'condition': {'if', 'check', 'condition', 'edge case', 'boundary'},\n",
        "    'early_return': {'return early', 'base case', 'edge case', 'special case'},\n",
        "\n",
        "    # Operations (4)\n",
        "    'count': {'count', 'frequency', 'occurrences', 'how many'},\n",
        "    'sum': {'sum', 'total', 'add up', 'accumulate'},\n",
        "    'max_min': {'maximum', 'minimum', 'max', 'min', 'largest', 'smallest'},\n",
        "    'string_op': {'string', 'character', 'substring', 'split', 'join'},\n",
        "}\n",
        "\n",
        "# ============================================================================\n",
        "# Unfaithfulness Patterns (from chainscope)\n",
        "# ============================================================================\n",
        "\n",
        "UNFAITHFUL_PATTERNS = {\n",
        "    'post_hoc': \"Reasoning constructed after answer decided\",\n",
        "    'restoration': \"Error acknowledged but not corrected\",\n",
        "    'shortcut': \"Conclusion without supporting steps\",\n",
        "    'phantom': \"Code construct not mentioned in reasoning\",\n",
        "    'filler': \"Reasoning steps that contribute nothing\",\n",
        "}\n",
        "\n",
        "# ============================================================================\n",
        "# Progress Tracker\n",
        "# ============================================================================\n",
        "\n",
        "POC_STATUS = {\n",
        "    \"cell_0\": \"✓ Design complete\",\n",
        "    \"cell_1\": \"⬜ Environment setup (Step 1)\",\n",
        "    \"cell_2\": \"⬜ Load CodeGemma (Step 2)\",\n",
        "    \"cell_3\": \"⬜ Load OpenThoughts (Step 3)\",\n",
        "    \"cell_4\": \"⬜ Load chainscope (Step 4)\",\n",
        "    \"cell_5\": \"⬜ CoT segmenter (Step 5)\",\n",
        "    \"cell_6\": \"⬜ CoT concepts (Step 6)\",\n",
        "    \"cell_7\": \"⬜ AST concepts (Step 7)\",\n",
        "    \"cell_8\": \"⬜ Reaching defs (Step 8)\",\n",
        "    \"cell_9\": \"⬜ Phantom ratio (Step 9)\",\n",
        "    \"cell_10\": \"⬜ Dead ratio (Step 10)\",\n",
        "    \"cell_11\": \"⬜ UniXcoder (Step 11)\",\n",
        "    \"cell_12\": \"⬜ Faithfulness (Step 12)\",\n",
        "    \"cell_13\": \"⬜ Execution harness (Step 13)\",\n",
        "    \"cell_14\": \"⬜ OpenThoughts analysis (Step 14)\",\n",
        "    \"cell_15\": \"⬜ HumanEval generation (Step 15)\",\n",
        "    \"cell_16\": \"⬜ Combine dataset (Step 16)\",\n",
        "    \"cell_17\": \"⬜ Correlation (Step 17)\",\n",
        "    \"cell_18\": \"⬜ Fisher's test (Step 18)\",\n",
        "    \"cell_19\": \"⬜ Effect sizes (Step 19)\",\n",
        "    \"cell_20\": \"⬜ Report (Step 20)\",\n",
        "}\n",
        "\n",
        "# ============================================================================\n",
        "# Phase Summary\n",
        "# ============================================================================\n",
        "\n",
        "PHASES = {\n",
        "    \"Phase 1\": {\n",
        "        \"name\": \"Environment & Data Setup\",\n",
        "        \"steps\": [1, 2, 3, 4],\n",
        "        \"time\": \"1.5 hours\",\n",
        "        \"outputs\": [\"Environment\", \"CodeGemma\", \"OpenThoughts\", \"Reference\"],\n",
        "    },\n",
        "    \"Phase 2\": {\n",
        "        \"name\": \"Extraction Pipeline\",\n",
        "        \"steps\": [5, 6, 7, 8],\n",
        "        \"time\": \"2.5 hours\",\n",
        "        \"outputs\": [\"segment_cot()\", \"extract_cot_concepts()\",\n",
        "                   \"extract_code_concepts()\", \"compute_reaching_defs()\"],\n",
        "    },\n",
        "    \"Phase 3\": {\n",
        "        \"name\": \"Metrics Development\",\n",
        "        \"steps\": [9, 10, 11, 12],\n",
        "        \"time\": \"2.5 hours\",\n",
        "        \"outputs\": [\"phantom_ratio()\", \"dead_ratio()\",\n",
        "                   \"semantic_similarity()\", \"compute_faithfulness()\"],\n",
        "    },\n",
        "    \"Phase 4\": {\n",
        "        \"name\": \"Evaluation Framework\",\n",
        "        \"steps\": [13, 14, 15, 16],\n",
        "        \"time\": \"3 hours\",\n",
        "        \"outputs\": [\"execute_code_safely()\", \"100 OT results\",\n",
        "                   \"50 HE results\", \"combined_df\"],\n",
        "    },\n",
        "    \"Phase 5\": {\n",
        "        \"name\": \"Statistical Analysis\",\n",
        "        \"steps\": [17, 18, 19, 20],\n",
        "        \"time\": \"2.5 hours\",\n",
        "        \"outputs\": [\"correlation_results\", \"fisher_results\",\n",
        "                   \"effect_results\", \"report + figures\"],\n",
        "    },\n",
        "}\n",
        "\n",
        "# ============================================================================\n",
        "# Print Summary\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"CELL 0 COMPLETE: Design finalized\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\n📊 DATASET: {CONFIG['dataset']['n_total']} samples\")\n",
        "print(f\"   ├── OpenThoughts-114k: {CONFIG['dataset']['n_primary']} samples\")\n",
        "print(f\"   └── HumanEval (CodeGemma): {CONFIG['dataset']['n_secondary']} samples\")\n",
        "\n",
        "print(f\"\\n🧠 MODEL: {CONFIG['model']['name']}\")\n",
        "print(f\"   └── Memory: ~{CONFIG['model']['memory_gb']} GB\")\n",
        "\n",
        "print(f\"\\n📐 CONCEPTS: {CONFIG['concepts']['n_concepts']} programming concepts\")\n",
        "print(f\"   └── Categories: {', '.join(CONFIG['concepts']['categories'])}\")\n",
        "\n",
        "print(f\"\\n📈 STATISTICS:\")\n",
        "print(f\"   ├── α = {CONFIG['statistics']['significance_level']}\")\n",
        "print(f\"   ├── Bootstrap: {CONFIG['statistics']['bootstrap_resamples']} resamples\")\n",
        "print(f\"   └── Target effect: d ≥ {CONFIG['statistics']['effect_size_target']}\")\n",
        "\n",
        "print(f\"\\n⏱️ TIMELINE: 15 hours across 5 phases\")\n",
        "for phase_name, phase_info in PHASES.items():\n",
        "    print(f\"   {phase_name}: {phase_info['name']} ({phase_info['time']})\")\n",
        "\n",
        "print(f\"\\n🎯 HYPOTHESIS: phantom_ratio correlates negatively with test pass rate\")\n",
        "print(f\"   PASS: r < 0, p < 0.05, d ≥ 0.5\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Proceed to Cell 1: Environment Setup\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "     CELL 1: ENVIRONMENT SETUP & AUTHENTICATION\n",
        "================================================================================\n",
        "\n",
        "OBJECTIVES:\n",
        "  ├── [1.1] Verify GPU availability (H100/A100/T4)\n",
        "  ├── [1.2] Install all dependencies\n",
        "  ├── [1.3] Configure API authentication (Kaggle, HuggingFace)\n",
        "  ├── [1.4] Import all libraries\n",
        "  └── [1.5] Validate environment\n",
        "\n",
        "COLAB SECRETS REQUIRED:\n",
        "  ├── KAGGLE_USERNAME  → Kaggle username for CodeGemma download\n",
        "  ├── KAGGLE_KEY       → Kaggle API key\n",
        "  └── HF_TOKEN         → HuggingFace token (optional, for gated models)\n",
        "\n",
        "TIME ESTIMATE: ~20 minutes (mostly pip install)\n",
        "\n",
        "================================================================================\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"CELL 1: ENVIRONMENT SETUP\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print()\n",
        "\n",
        "# ============================================================================\n",
        "# [1.1] GPU VERIFICATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"[1/5] GPU Verification...\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "try:\n",
        "    import subprocess\n",
        "    result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total,driver_version',\n",
        "                           '--format=csv,noheader'], capture_output=True, text=True)\n",
        "    if result.returncode == 0:\n",
        "        gpu_info = result.stdout.strip()\n",
        "        print(f\"  ✅ GPU detected: {gpu_info}\")\n",
        "\n",
        "        # Check for recommended GPUs\n",
        "        if 'H100' in gpu_info:\n",
        "            print(\"  🚀 H100 detected - optimal for this notebook\")\n",
        "            GPU_TYPE = \"H100\"\n",
        "        elif 'A100' in gpu_info:\n",
        "            print(\"  🚀 A100 detected - excellent for this notebook\")\n",
        "            GPU_TYPE = \"A100\"\n",
        "        elif 'V100' in gpu_info:\n",
        "            print(\"  ✅ V100 detected - good for this notebook\")\n",
        "            GPU_TYPE = \"V100\"\n",
        "        elif 'T4' in gpu_info:\n",
        "            print(\"  ⚠️  T4 detected - may need to skip CodeGemma generation\")\n",
        "            print(\"      → Will use OpenThoughts only (100 samples)\")\n",
        "            GPU_TYPE = \"T4\"\n",
        "        elif 'L4' in gpu_info:\n",
        "            print(\"  ✅ L4 detected - good for this notebook\")\n",
        "            GPU_TYPE = \"L4\"\n",
        "        else:\n",
        "            print(\"  ✅ GPU detected - proceeding\")\n",
        "            GPU_TYPE = \"OTHER\"\n",
        "    else:\n",
        "        print(\"  ❌ No GPU detected\")\n",
        "        print(\"  → Falling back to CPU (will be slow)\")\n",
        "        GPU_TYPE = \"CPU\"\n",
        "except Exception as e:\n",
        "    print(f\"  ❌ GPU check failed: {e}\")\n",
        "    GPU_TYPE = \"CPU\"\n",
        "\n",
        "print()\n",
        "\n",
        "# ============================================================================\n",
        "# [1.2] DEPENDENCY INSTALLATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"[2/5] Installing Dependencies...\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Core dependencies for CoT-DFA\n",
        "DEPENDENCIES = {\n",
        "    \"core\": [\n",
        "        \"kagglehub\",           # CodeGemma download\n",
        "        \"transformers>=4.40.0\", # Models and tokenizers\n",
        "        \"datasets\",            # HuggingFace datasets\n",
        "        \"sentencepiece\",       # Tokenization\n",
        "        \"accelerate\",          # Model loading utilities\n",
        "    ],\n",
        "    \"analysis\": [\n",
        "        \"scipy\",               # Statistical tests\n",
        "        \"pandas\",              # DataFrames\n",
        "        \"numpy\",               # Numerics\n",
        "        \"beniget\",             # AST def-use chains\n",
        "    ],\n",
        "    \"visualization\": [\n",
        "        \"matplotlib\",          # Plots\n",
        "        \"seaborn\",             # Statistical visualization\n",
        "        \"tqdm\",                # Progress bars\n",
        "    ],\n",
        "    \"optional_jax\": [\n",
        "        # Uncomment if using JAX/Flax for CodeGemma\n",
        "        # \"jax[cuda12]\",\n",
        "        # \"flax\",\n",
        "        # \"gemma\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "def install_packages(packages, category):\n",
        "    \"\"\"Install a list of packages with progress reporting.\"\"\"\n",
        "    import subprocess\n",
        "    for pkg in packages:\n",
        "        if not pkg or pkg.startswith('#'):\n",
        "            continue\n",
        "        pkg_name = pkg.split('>=')[0].split('==')[0]\n",
        "        print(f\"  Installing {pkg_name}...\", end=\" \", flush=True)\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg],\n",
        "                capture_output=True, text=True\n",
        "            )\n",
        "            if result.returncode == 0:\n",
        "                print(\"✅\")\n",
        "            else:\n",
        "                print(f\"⚠️ ({result.stderr.strip()[:50]})\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ {e}\")\n",
        "\n",
        "# Install each category\n",
        "for category, packages in DEPENDENCIES.items():\n",
        "    if packages and not packages[0].startswith('#'):\n",
        "        print(f\"\\n  [{category.upper()}]\")\n",
        "        install_packages(packages, category)\n",
        "\n",
        "print()\n",
        "\n",
        "# ============================================================================\n",
        "# [1.3] API AUTHENTICATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"[3/5] API Authentication...\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Track authentication status\n",
        "AUTH_STATUS = {\n",
        "    \"kaggle\": False,\n",
        "    \"huggingface\": False,\n",
        "}\n",
        "\n",
        "# --- Kaggle Authentication (required for CodeGemma) ---\n",
        "print(\"\\n  [KAGGLE] - Required for CodeGemma download\")\n",
        "\n",
        "# Configuration - UPDATE THESE IF NEEDED\n",
        "KAGGLE_USERNAME = \"shakthibachala\"  # Your Kaggle username\n",
        "KAGGLE_SECRET_NAME = \"KAGGLE_API_TOKEN\"  # Name of secret containing API key\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "\n",
        "    try:\n",
        "        # Get API key from Colab Secrets\n",
        "        kaggle_key = userdata.get(KAGGLE_SECRET_NAME)\n",
        "\n",
        "        if kaggle_key:\n",
        "            # Set environment variables\n",
        "            os.environ[\"KAGGLE_USERNAME\"] = KAGGLE_USERNAME\n",
        "            os.environ[\"KAGGLE_KEY\"] = kaggle_key\n",
        "\n",
        "            # Also write kaggle.json for libraries that need it\n",
        "            kaggle_dir = os.path.expanduser(\"~/.kaggle\")\n",
        "            os.makedirs(kaggle_dir, exist_ok=True)\n",
        "            kaggle_json_path = os.path.join(kaggle_dir, \"kaggle.json\")\n",
        "\n",
        "            import json\n",
        "            with open(kaggle_json_path, 'w') as f:\n",
        "                json.dump({\"username\": KAGGLE_USERNAME, \"key\": kaggle_key}, f)\n",
        "            os.chmod(kaggle_json_path, 0o600)  # Secure permissions\n",
        "\n",
        "            print(f\"  ✅ Kaggle authenticated as: {KAGGLE_USERNAME}\")\n",
        "            print(f\"  ✅ Created ~/.kaggle/kaggle.json\")\n",
        "            AUTH_STATUS[\"kaggle\"] = True\n",
        "        else:\n",
        "            raise ValueError(\"Secret returned empty\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # Check if kaggle.json already exists\n",
        "        kaggle_json_path = os.path.expanduser(\"~/.kaggle/kaggle.json\")\n",
        "        if os.path.exists(kaggle_json_path):\n",
        "            print(f\"  ✅ Kaggle authenticated via existing ~/.kaggle/kaggle.json\")\n",
        "            AUTH_STATUS[\"kaggle\"] = True\n",
        "        else:\n",
        "            print(f\"  ⚠️  Kaggle credentials not found: {e}\")\n",
        "            print(f\"  → Add '{KAGGLE_SECRET_NAME}' to Colab Secrets (your API key)\")\n",
        "            print(f\"  → Get key from: https://www.kaggle.com/settings → API → Create New Token\")\n",
        "            print(\"  → CodeGemma generation will be skipped\")\n",
        "\n",
        "except ImportError:\n",
        "    # Not in Colab\n",
        "    print(\"  ℹ️  Not running in Colab, checking local Kaggle config...\")\n",
        "    kaggle_json = os.path.expanduser(\"~/.kaggle/kaggle.json\")\n",
        "    if os.path.exists(kaggle_json):\n",
        "        print(f\"  ✅ Kaggle authenticated via {kaggle_json}\")\n",
        "        AUTH_STATUS[\"kaggle\"] = True\n",
        "    else:\n",
        "        print(\"  ⚠️  No Kaggle credentials found\")\n",
        "\n",
        "# --- HuggingFace Authentication (optional, for gated models) ---\n",
        "print(\"\\n  [HUGGINGFACE] - Optional for gated models\")\n",
        "\n",
        "# Configuration - UPDATE THESE IF NEEDED\n",
        "HF_SECRET_NAME = \"mech_interp\"  # Your HF token secret name\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "\n",
        "    try:\n",
        "        hf_token = userdata.get(HF_SECRET_NAME)\n",
        "        if hf_token:\n",
        "            os.environ[\"HF_TOKEN\"] = hf_token\n",
        "            os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = hf_token\n",
        "\n",
        "            # Login to HuggingFace\n",
        "            try:\n",
        "                from huggingface_hub import login\n",
        "                login(token=hf_token, add_to_git_credential=False)\n",
        "                print(f\"  ✅ HuggingFace authenticated (via '{HF_SECRET_NAME}' secret)\")\n",
        "                AUTH_STATUS[\"huggingface\"] = True\n",
        "            except Exception as e:\n",
        "                print(f\"  ⚠️  HuggingFace login failed: {e}\")\n",
        "        else:\n",
        "            print(f\"  ℹ️  No '{HF_SECRET_NAME}' secret found (optional)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ℹ️  HuggingFace token not configured: {e}\")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"  ℹ️  Not in Colab, skipping HF authentication\")\n",
        "\n",
        "print()\n",
        "\n",
        "# ============================================================================\n",
        "# [1.4] IMPORT ALL LIBRARIES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"[4/5] Importing Libraries...\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Track import status\n",
        "IMPORT_STATUS = {}\n",
        "\n",
        "def safe_import(module_name, alias=None, from_module=None, import_items=None):\n",
        "    \"\"\"Safely import a module with error handling.\"\"\"\n",
        "    try:\n",
        "        if from_module:\n",
        "            module = __import__(from_module, fromlist=[module_name])\n",
        "            obj = getattr(module, module_name)\n",
        "            globals()[alias or module_name] = obj\n",
        "        elif import_items:\n",
        "            module = __import__(module_name)\n",
        "            for item in import_items:\n",
        "                globals()[item] = getattr(module, item)\n",
        "        else:\n",
        "            module = __import__(module_name)\n",
        "            globals()[alias or module_name] = module\n",
        "        IMPORT_STATUS[alias or module_name] = True\n",
        "        return True\n",
        "    except ImportError as e:\n",
        "        IMPORT_STATUS[alias or module_name] = False\n",
        "        return False\n",
        "\n",
        "# --- Core Libraries ---\n",
        "print(\"\\n  [CORE]\")\n",
        "imports_core = [\n",
        "    (\"os\", None),\n",
        "    (\"sys\", None),\n",
        "    (\"re\", None),\n",
        "    (\"ast\", None),\n",
        "    (\"json\", None),\n",
        "    (\"time\", None),\n",
        "    (\"subprocess\", None),\n",
        "    (\"tempfile\", None),\n",
        "    (\"dataclasses\", None),\n",
        "]\n",
        "for module, alias in imports_core:\n",
        "    if safe_import(module, alias):\n",
        "        print(f\"    ✅ {module}\")\n",
        "    else:\n",
        "        print(f\"    ❌ {module}\")\n",
        "\n",
        "# --- Data Science ---\n",
        "print(\"\\n  [DATA SCIENCE]\")\n",
        "imports_ds = [\n",
        "    (\"numpy\", \"np\"),\n",
        "    (\"pandas\", \"pd\"),\n",
        "    (\"scipy\", None),\n",
        "    (\"scipy.stats\", \"stats\"),\n",
        "]\n",
        "for module, alias in imports_ds:\n",
        "    if safe_import(module, alias):\n",
        "        print(f\"    ✅ {alias or module}\")\n",
        "    else:\n",
        "        print(f\"    ❌ {alias or module}\")\n",
        "\n",
        "# --- ML/NLP ---\n",
        "print(\"\\n  [ML/NLP]\")\n",
        "ml_imports = [\n",
        "    \"transformers\",\n",
        "    \"datasets\",\n",
        "    \"torch\",\n",
        "]\n",
        "for module in ml_imports:\n",
        "    if safe_import(module):\n",
        "        print(f\"    ✅ {module}\")\n",
        "    else:\n",
        "        print(f\"    ❌ {module}\")\n",
        "\n",
        "# --- AST Analysis ---\n",
        "print(\"\\n  [AST ANALYSIS]\")\n",
        "if safe_import(\"beniget\"):\n",
        "    print(\"    ✅ beniget\")\n",
        "else:\n",
        "    print(\"    ⚠️  beniget not available (will use basic AST)\")\n",
        "\n",
        "# --- Visualization ---\n",
        "print(\"\\n  [VISUALIZATION]\")\n",
        "viz_imports = [\n",
        "    (\"matplotlib.pyplot\", \"plt\"),\n",
        "    (\"seaborn\", \"sns\"),\n",
        "]\n",
        "for module, alias in viz_imports:\n",
        "    try:\n",
        "        exec(f\"import {module} as {alias}\")\n",
        "        globals()[alias] = eval(alias)\n",
        "        IMPORT_STATUS[alias] = True\n",
        "        print(f\"    ✅ {alias}\")\n",
        "    except ImportError:\n",
        "        IMPORT_STATUS[alias] = False\n",
        "        print(f\"    ❌ {alias}\")\n",
        "\n",
        "# --- Progress Bars ---\n",
        "if safe_import(\"tqdm\"):\n",
        "    from tqdm.auto import tqdm\n",
        "    print(\"    ✅ tqdm\")\n",
        "else:\n",
        "    # Fallback tqdm\n",
        "    def tqdm(iterable, **kwargs):\n",
        "        return iterable\n",
        "    print(\"    ⚠️  tqdm (using fallback)\")\n",
        "\n",
        "# --- Kagglehub ---\n",
        "print(\"\\n  [MODEL DOWNLOAD]\")\n",
        "if safe_import(\"kagglehub\"):\n",
        "    print(\"    ✅ kagglehub\")\n",
        "else:\n",
        "    print(\"    ❌ kagglehub (CodeGemma download will fail)\")\n",
        "\n",
        "print()\n",
        "\n",
        "# ============================================================================\n",
        "# [1.5] ENVIRONMENT VALIDATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"[5/5] Environment Validation...\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Collect environment info\n",
        "ENV_INFO = {\n",
        "    \"python_version\": sys.version.split()[0],\n",
        "    \"gpu_type\": GPU_TYPE,\n",
        "    \"kaggle_auth\": AUTH_STATUS[\"kaggle\"],\n",
        "    \"hf_auth\": AUTH_STATUS[\"huggingface\"],\n",
        "    \"torch_available\": IMPORT_STATUS.get(\"torch\", False),\n",
        "    \"transformers_available\": IMPORT_STATUS.get(\"transformers\", False),\n",
        "    \"datasets_available\": IMPORT_STATUS.get(\"datasets\", False),\n",
        "    \"kagglehub_available\": IMPORT_STATUS.get(\"kagglehub\", False),\n",
        "    \"scipy_available\": IMPORT_STATUS.get(\"scipy\", False),\n",
        "    \"beniget_available\": IMPORT_STATUS.get(\"beniget\", False),\n",
        "}\n",
        "\n",
        "# Print summary\n",
        "print(f\"\\n  Python:        {ENV_INFO['python_version']}\")\n",
        "print(f\"  GPU:           {ENV_INFO['gpu_type']}\")\n",
        "print(f\"  Kaggle Auth:   {'✅' if ENV_INFO['kaggle_auth'] else '❌'}\")\n",
        "print(f\"  HF Auth:       {'✅' if ENV_INFO['hf_auth'] else 'ℹ️ (optional)'}\")\n",
        "print(f\"  PyTorch:       {'✅' if ENV_INFO['torch_available'] else '❌'}\")\n",
        "print(f\"  Transformers:  {'✅' if ENV_INFO['transformers_available'] else '❌'}\")\n",
        "print(f\"  Datasets:      {'✅' if ENV_INFO['datasets_available'] else '❌'}\")\n",
        "print(f\"  Kagglehub:     {'✅' if ENV_INFO['kagglehub_available'] else '❌'}\")\n",
        "print(f\"  SciPy:         {'✅' if ENV_INFO['scipy_available'] else '❌'}\")\n",
        "print(f\"  Beniget:       {'✅' if ENV_INFO['beniget_available'] else '⚠️ (fallback)'}\")\n",
        "\n",
        "# Determine capabilities\n",
        "print(\"\\n  [CAPABILITIES]\")\n",
        "CAN_GENERATE_CODEGEMMA = (\n",
        "    ENV_INFO['kaggle_auth'] and\n",
        "    ENV_INFO['kagglehub_available'] and\n",
        "    ENV_INFO['gpu_type'] in ['H100', 'A100', 'V100', 'L4']\n",
        ")\n",
        "CAN_USE_OPENTHOUGHTS = (\n",
        "    ENV_INFO['datasets_available'] and\n",
        "    ENV_INFO['transformers_available']\n",
        ")\n",
        "CAN_USE_UNIXCODER = (\n",
        "    ENV_INFO['torch_available'] and\n",
        "    ENV_INFO['transformers_available']\n",
        ")\n",
        "CAN_RUN_STATISTICS = ENV_INFO['scipy_available']\n",
        "\n",
        "print(f\"  CodeGemma Generation:  {'✅ Available' if CAN_GENERATE_CODEGEMMA else '❌ Skipped'}\")\n",
        "print(f\"  OpenThoughts Dataset:  {'✅ Available' if CAN_USE_OPENTHOUGHTS else '❌ Missing deps'}\")\n",
        "print(f\"  UniXcoder Embeddings:  {'✅ Available' if CAN_USE_UNIXCODER else '❌ Missing deps'}\")\n",
        "print(f\"  Statistical Analysis:  {'✅ Available' if CAN_RUN_STATISTICS else '❌ Missing scipy'}\")\n",
        "\n",
        "# Warnings\n",
        "if not CAN_GENERATE_CODEGEMMA:\n",
        "    print(\"\\n  ⚠️  CodeGemma generation disabled:\")\n",
        "    if not ENV_INFO['kaggle_auth']:\n",
        "        print(\"      → Missing Kaggle credentials\")\n",
        "    if not ENV_INFO['kagglehub_available']:\n",
        "        print(\"      → kagglehub not installed\")\n",
        "    if ENV_INFO['gpu_type'] == 'T4':\n",
        "        print(\"      → T4 GPU has limited VRAM\")\n",
        "    print(\"      → Will use OpenThoughts samples only (100 samples)\")\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION OBJECT FOR DOWNSTREAM CELLS\n",
        "# ============================================================================\n",
        "\n",
        "class EnvironmentConfig:\n",
        "    \"\"\"Configuration object passed to downstream cells.\"\"\"\n",
        "\n",
        "    # Environment\n",
        "    PYTHON_VERSION = ENV_INFO['python_version']\n",
        "    GPU_TYPE = GPU_TYPE\n",
        "\n",
        "    # Authentication\n",
        "    KAGGLE_AUTH = AUTH_STATUS['kaggle']\n",
        "    HF_AUTH = AUTH_STATUS['huggingface']\n",
        "\n",
        "    # Capabilities\n",
        "    CAN_GENERATE_CODEGEMMA = CAN_GENERATE_CODEGEMMA\n",
        "    CAN_USE_OPENTHOUGHTS = CAN_USE_OPENTHOUGHTS\n",
        "    CAN_USE_UNIXCODER = CAN_USE_UNIXCODER\n",
        "    CAN_RUN_STATISTICS = CAN_RUN_STATISTICS\n",
        "\n",
        "    # Dataset configuration (adjusted based on capabilities)\n",
        "    N_OPENTHOUGHTS = 100\n",
        "    N_HUMANEVAL = 50 if CAN_GENERATE_CODEGEMMA else 0\n",
        "    N_TOTAL = N_OPENTHOUGHTS + N_HUMANEVAL\n",
        "\n",
        "    # Execution\n",
        "    RANDOM_SEED = 42\n",
        "    EXECUTION_TIMEOUT = 10  # seconds\n",
        "\n",
        "    # Statistics\n",
        "    SIGNIFICANCE_LEVEL = 0.05\n",
        "    BOOTSTRAP_RESAMPLES = 9999\n",
        "    FAITHFULNESS_ALPHA = 0.7  # Structural weight\n",
        "    FAITHFULNESS_BETA = 0.3   # Semantic weight\n",
        "\n",
        "# Create config instance\n",
        "ENV_CONFIG = EnvironmentConfig()\n",
        "\n",
        "# ============================================================================\n",
        "# FINAL STATUS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"CELL 1 COMPLETE: Environment configured\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Determine overall status\n",
        "critical_missing = []\n",
        "if not CAN_USE_OPENTHOUGHTS:\n",
        "    critical_missing.append(\"datasets/transformers\")\n",
        "if not CAN_RUN_STATISTICS:\n",
        "    critical_missing.append(\"scipy\")\n",
        "\n",
        "if critical_missing:\n",
        "    print(f\"\\n❌ CRITICAL: Missing {', '.join(critical_missing)}\")\n",
        "    print(\"   Cannot proceed - please fix dependencies\")\n",
        "else:\n",
        "    print(f\"\\n✅ Ready to proceed!\")\n",
        "    print(f\"   Dataset size: {ENV_CONFIG.N_TOTAL} samples\")\n",
        "    if not CAN_GENERATE_CODEGEMMA:\n",
        "        print(f\"   Note: Using OpenThoughts only (CodeGemma disabled)\")\n",
        "\n",
        "print(f\"\\n📌 ENV_CONFIG object available for downstream cells\")\n",
        "print(f\"   Access: ENV_CONFIG.CAN_GENERATE_CODEGEMMA, etc.\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Proceed to Cell 2: Load CodeGemma (or skip if disabled)\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "id": "eRC0EQFo4eDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "CELL 2: MODEL SETUP\n",
        "===================\n",
        "OpenThoughts-114k already has DeepSeek-R1 reasoning traces.\n",
        "CodeGemma is NOT a reasoning model (no native <think> tags).\n",
        "\n",
        "This cell just validates we can do inference if needed later.\n",
        "Primary data comes from OpenThoughts (Cell 3).\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CELL 2: Model Setup\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============================================================================\n",
        "# REASONING MODEL CONTEXT\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\"\"\n",
        "┌─────────────────────────────────────────────────────────────┐\n",
        "│  REASONING MODELS vs CODE MODELS                            │\n",
        "├─────────────────────────────────────────────────────────────┤\n",
        "│                                                             │\n",
        "│  DeepSeek-R1 (reasoning):  Native <think>...</think> tags  │\n",
        "│  ├── Used in OpenThoughts-114k dataset                     │\n",
        "│  └── TRUE chain-of-thought reasoning                       │\n",
        "│                                                             │\n",
        "│  CodeGemma (code):  Standard code generation               │\n",
        "│  ├── No native reasoning format                            │\n",
        "│  └── Would need prompting to fake CoT                      │\n",
        "│                                                             │\n",
        "│  DECISION: Use OpenThoughts (real reasoning) only          │\n",
        "│                                                             │\n",
        "└─────────────────────────────────────────────────────────────┘\n",
        "\"\"\")\n",
        "\n",
        "# ============================================================================\n",
        "# GPU CHECK\n",
        "# ============================================================================\n",
        "\n",
        "print(\"[1/2] GPU Status...\")\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"  ✅ GPU: {gpu_name} ({gpu_mem:.0f} GB)\")\n",
        "    DEVICE = \"cuda\"\n",
        "else:\n",
        "    print(\"  ⚠️  No GPU, using CPU\")\n",
        "    DEVICE = \"cpu\"\n",
        "\n",
        "# ============================================================================\n",
        "# UPDATE CONFIG\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[2/2] Updating configuration...\")\n",
        "\n",
        "# Override previous config - we're using OpenThoughts only\n",
        "ENV_CONFIG.CAN_GENERATE_CODEGEMMA = False\n",
        "ENV_CONFIG.N_HUMANEVAL = 0\n",
        "ENV_CONFIG.N_OPENTHOUGHTS = 150  # Increased since no HumanEval\n",
        "ENV_CONFIG.N_TOTAL = 150\n",
        "\n",
        "print(f\"  ✅ Dataset: OpenThoughts-114k only\")\n",
        "print(f\"  ✅ Sample size: {ENV_CONFIG.N_TOTAL} samples\")\n",
        "print(f\"  ✅ Source: DeepSeek-R1 reasoning traces (real CoT)\")\n",
        "\n",
        "# ============================================================================\n",
        "# EXPORTS\n",
        "# ============================================================================\n",
        "\n",
        "class ModelExports:\n",
        "    \"\"\"Model configuration for downstream cells.\"\"\"\n",
        "    device: str = DEVICE\n",
        "    available: bool = True\n",
        "    source: str = \"OpenThoughts-114k (DeepSeek-R1)\"\n",
        "\n",
        "MODEL_CONFIG = ModelExports()\n",
        "\n",
        "# ============================================================================\n",
        "# SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"✅ CELL 2 COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\"\"\n",
        "Data Strategy:\n",
        "  ├── Source: OpenThoughts-114k\n",
        "  ├── Model: DeepSeek-R1 (true reasoning model)\n",
        "  ├── Format: Native <think>...</think> tags\n",
        "  ├── Samples: {ENV_CONFIG.N_TOTAL}\n",
        "  └── Quality: High (real chain-of-thought)\n",
        "\n",
        "Why not CodeGemma?\n",
        "  └── Not a reasoning model - would need fake prompting\n",
        "\n",
        "Proceed to Cell 3: Load OpenThoughts Dataset\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "zHmlcR0M8c02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "CELL 3: LOAD OPENTHOUGHTS DATASET\n",
        "=================================\n",
        "Load DeepSeek-R1 reasoning traces for code generation problems.\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Set, Optional, Tuple\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CELL 3: Load OpenThoughts Dataset\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============================================================================\n",
        "# LOAD DATASET\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[1/4] Loading OpenThoughts-114k...\")\n",
        "\n",
        "ds = load_dataset(\"open-thoughts/OpenThoughts-114k\", split=\"train\")\n",
        "print(f\"  ✅ Loaded {len(ds):,} total samples\")\n",
        "\n",
        "# Inspect structure\n",
        "print(f\"\\n  Dataset columns: {ds.column_names}\")\n",
        "sample_ex = ds[0]\n",
        "print(f\"  Sample keys: {list(sample_ex.keys())}\")\n",
        "\n",
        "# Show sample values for key fields\n",
        "for key in list(sample_ex.keys())[:6]:\n",
        "    val = str(sample_ex[key])[:100] if sample_ex[key] else \"None\"\n",
        "    print(f\"    {key}: {val}...\")\n",
        "\n",
        "# ============================================================================\n",
        "# FILTER FOR CODE DOMAIN\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[2/4] Filtering for code problems...\")\n",
        "\n",
        "# Identify the actual field names\n",
        "SOLUTION_FIELDS = ['deepseek_solution', 'solution', 'response', 'answer', 'output']\n",
        "REASONING_FIELDS = ['deepseek_reasoning', 'reasoning', 'thought', 'thinking']\n",
        "SOURCE_FIELDS = ['source', 'dataset', 'domain', 'category']\n",
        "\n",
        "def get_field(example, field_names):\n",
        "    \"\"\"Get first available field from list.\"\"\"\n",
        "    for field in field_names:\n",
        "        if field in example and example[field]:\n",
        "            return example[field]\n",
        "    return \"\"\n",
        "\n",
        "def is_code_sample(example):\n",
        "    \"\"\"Check if sample contains code.\"\"\"\n",
        "    # Get solution from various possible fields\n",
        "    solution = get_field(example, SOLUTION_FIELDS)\n",
        "\n",
        "    # Check if solution looks like Python code\n",
        "    code_indicators = ['def ', 'class ', 'import ', 'return ', 'for ', 'while ', 'if ']\n",
        "    if any(ind in solution for ind in code_indicators):\n",
        "        return True\n",
        "\n",
        "    # Check source/domain field\n",
        "    source = get_field(example, SOURCE_FIELDS).lower()\n",
        "    code_sources = ['taco', 'apps', 'code', 'python', 'leetcode', 'contest']\n",
        "    if any(s in source for s in code_sources):\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "# Filter with progress\n",
        "print(\"  Scanning for code samples...\")\n",
        "code_samples = []\n",
        "for i, ex in enumerate(ds):\n",
        "    if is_code_sample(ex):\n",
        "        code_samples.append(ex)\n",
        "    if i % 20000 == 0:\n",
        "        print(f\"    Scanned {i:,}... found {len(code_samples):,} code samples\")\n",
        "\n",
        "print(f\"  ✅ Found {len(code_samples):,} code samples\")\n",
        "\n",
        "# ============================================================================\n",
        "# PARSE AND VALIDATE SAMPLES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[3/4] Parsing samples...\")\n",
        "\n",
        "@dataclass\n",
        "class CodeSample:\n",
        "    \"\"\"A parsed code sample with reasoning trace.\"\"\"\n",
        "    id: str\n",
        "    problem: str\n",
        "    thinking: str\n",
        "    solution: str\n",
        "    source: str\n",
        "    test_cases: Optional[str] = None\n",
        "\n",
        "    @property\n",
        "    def has_thinking(self) -> bool:\n",
        "        return len(self.thinking) > 50\n",
        "\n",
        "    @property\n",
        "    def has_solution(self) -> bool:\n",
        "        return 'def ' in self.solution or 'class ' in self.solution\n",
        "\n",
        "    @property\n",
        "    def is_valid(self) -> bool:\n",
        "        return self.has_thinking and self.has_solution\n",
        "\n",
        "def extract_thinking(text: str) -> str:\n",
        "    \"\"\"Extract content from <think>...</think> tags.\"\"\"\n",
        "    # Try explicit tags first\n",
        "    match = re.search(r'<think>(.*?)</think>', text, re.DOTALL | re.IGNORECASE)\n",
        "    if match:\n",
        "        return match.group(1).strip()\n",
        "\n",
        "    # Try other common patterns\n",
        "    for pattern in [\n",
        "        r'<reasoning>(.*?)</reasoning>',\n",
        "        r'<thought>(.*?)</thought>',\n",
        "        r'\\*\\*Thinking\\*\\*:?\\s*(.*?)(?=\\*\\*|```|$)',\n",
        "    ]:\n",
        "        match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)\n",
        "        if match:\n",
        "            return match.group(1).strip()\n",
        "\n",
        "    # Fallback: everything before code block\n",
        "    code_start = text.find('```')\n",
        "    if code_start > 100:\n",
        "        return text[:code_start].strip()\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "def extract_code(text: str) -> str:\n",
        "    \"\"\"Extract Python code from solution.\"\"\"\n",
        "    # Try ```python block\n",
        "    match = re.search(r'```python\\s*(.*?)```', text, re.DOTALL)\n",
        "    if match:\n",
        "        return match.group(1).strip()\n",
        "\n",
        "    # Try any ``` block\n",
        "    match = re.search(r'```\\s*(.*?)```', text, re.DOTALL)\n",
        "    if match:\n",
        "        code = match.group(1).strip()\n",
        "        if 'def ' in code or 'class ' in code:\n",
        "            return code\n",
        "\n",
        "    # Try to find raw code\n",
        "    lines = text.split('\\n')\n",
        "    code_lines = []\n",
        "    in_code = False\n",
        "\n",
        "    for line in lines:\n",
        "        stripped = line.strip()\n",
        "        if stripped.startswith(('def ', 'class ', 'import ', 'from ')):\n",
        "            in_code = True\n",
        "        if in_code:\n",
        "            if stripped and not stripped.startswith('#'):\n",
        "                code_lines.append(line)\n",
        "            elif not stripped and code_lines:\n",
        "                code_lines.append(line)\n",
        "\n",
        "    return '\\n'.join(code_lines).strip()\n",
        "\n",
        "def extract_test_cases(problem: str, solution: str) -> Optional[str]:\n",
        "    \"\"\"Try to extract test cases from problem or solution.\"\"\"\n",
        "    # Look for assert statements\n",
        "    asserts = re.findall(r'assert\\s+.+', solution)\n",
        "    if asserts:\n",
        "        return '\\n'.join(asserts[:5])\n",
        "\n",
        "    # Look for example outputs in problem\n",
        "    examples = re.findall(r'(?:Example|Input|Output).*?(?=Example|Input|$)',\n",
        "                          problem, re.DOTALL | re.IGNORECASE)\n",
        "    if examples:\n",
        "        return '\\n'.join(examples[:3])\n",
        "\n",
        "    return None\n",
        "\n",
        "def parse_sample(idx: int, example: dict) -> CodeSample:\n",
        "    \"\"\"Parse a raw example into CodeSample.\"\"\"\n",
        "    problem = get_field(example, ['problem', 'question', 'prompt', 'input'])\n",
        "    reasoning = get_field(example, REASONING_FIELDS)\n",
        "    solution = get_field(example, SOLUTION_FIELDS)\n",
        "    source = get_field(example, SOURCE_FIELDS) or 'unknown'\n",
        "\n",
        "    thinking = extract_thinking(reasoning) if reasoning else \"\"\n",
        "    code = extract_code(solution) if solution else \"\"\n",
        "    tests = extract_test_cases(problem, solution)\n",
        "\n",
        "    return CodeSample(\n",
        "        id=f\"ot_{idx:04d}\",\n",
        "        problem=problem,\n",
        "        thinking=thinking,\n",
        "        solution=code,\n",
        "        source=source,\n",
        "        test_cases=tests,\n",
        "    )\n",
        "\n",
        "# Parse all code samples\n",
        "parsed_samples = [parse_sample(i, ex) for i, ex in enumerate(code_samples)]\n",
        "valid_samples = [s for s in parsed_samples if s.is_valid]\n",
        "\n",
        "print(f\"  ✅ Parsed {len(parsed_samples):,} samples\")\n",
        "print(f\"  ✅ Valid (has thinking + code): {len(valid_samples):,}\")\n",
        "\n",
        "# If no valid samples, show debug info\n",
        "if len(valid_samples) == 0 and len(parsed_samples) > 0:\n",
        "    print(\"\\n  ⚠️  No valid samples! Debugging first parsed sample:\")\n",
        "    s = parsed_samples[0]\n",
        "    print(f\"    thinking length: {len(s.thinking)}\")\n",
        "    print(f\"    solution length: {len(s.solution)}\")\n",
        "    print(f\"    has_thinking: {s.has_thinking}\")\n",
        "    print(f\"    has_solution: {s.has_solution}\")\n",
        "    if s.thinking:\n",
        "        print(f\"    thinking preview: {s.thinking[:200]}...\")\n",
        "    if s.solution:\n",
        "        print(f\"    solution preview: {s.solution[:200]}...\")\n",
        "\n",
        "# ============================================================================\n",
        "# SELECT FINAL SAMPLE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[4/4] Selecting samples...\")\n",
        "\n",
        "import random\n",
        "random.seed(ENV_CONFIG.RANDOM_SEED)\n",
        "\n",
        "if len(valid_samples) == 0:\n",
        "    print(\"  ❌ No valid samples found!\")\n",
        "    print(\"\\n  Falling back: using samples with ANY code (relaxed validation)\")\n",
        "    # Relax validation - just need some code\n",
        "    valid_samples = [s for s in parsed_samples if len(s.solution) > 20]\n",
        "    print(f\"  ✅ Relaxed: {len(valid_samples)} samples with code\")\n",
        "\n",
        "if len(valid_samples) == 0:\n",
        "    print(\"  ❌ Still no samples! Using raw examples directly...\")\n",
        "    # Last resort: just take samples that have 'def ' in any field\n",
        "    for i, ex in enumerate(code_samples[:200]):\n",
        "        thinking = str(ex.get('deepseek_reasoning', ex.get('reasoning', '')))\n",
        "        solution = str(ex.get('deepseek_solution', ex.get('solution', '')))\n",
        "        if 'def ' in solution:\n",
        "            valid_samples.append(CodeSample(\n",
        "                id=f\"ot_{i:04d}\",\n",
        "                problem=str(ex.get('problem', ''))[:1000],\n",
        "                thinking=thinking,\n",
        "                solution=solution,\n",
        "                source=str(ex.get('source', 'unknown')),\n",
        "            ))\n",
        "    print(f\"  ✅ Direct extraction: {len(valid_samples)} samples\")\n",
        "\n",
        "N_SAMPLES = min(ENV_CONFIG.N_TOTAL, len(valid_samples))\n",
        "SAMPLES = random.sample(valid_samples, N_SAMPLES) if valid_samples else []\n",
        "\n",
        "# Sort by ID for reproducibility\n",
        "SAMPLES.sort(key=lambda x: x.id)\n",
        "\n",
        "print(f\"  ✅ Selected {N_SAMPLES} samples\")\n",
        "\n",
        "# ============================================================================\n",
        "# CREATE DATAFRAME\n",
        "# ============================================================================\n",
        "\n",
        "if SAMPLES:\n",
        "    SAMPLES_DF = pd.DataFrame([\n",
        "        {\n",
        "            'id': s.id,\n",
        "            'problem': s.problem[:500],\n",
        "            'thinking': s.thinking,\n",
        "            'solution': s.solution,\n",
        "            'source': s.source,\n",
        "            'thinking_len': len(s.thinking),\n",
        "            'solution_len': len(s.solution),\n",
        "            'has_tests': s.test_cases is not None,\n",
        "        }\n",
        "        for s in SAMPLES\n",
        "    ])\n",
        "else:\n",
        "    SAMPLES_DF = pd.DataFrame(columns=['id', 'problem', 'thinking', 'solution',\n",
        "                                        'source', 'thinking_len', 'solution_len', 'has_tests'])\n",
        "\n",
        "# ============================================================================\n",
        "# STATISTICS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Dataset Statistics:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "if len(SAMPLES) > 0:\n",
        "    stats = {\n",
        "        'Total samples': len(SAMPLES),\n",
        "        'Avg thinking length': f\"{SAMPLES_DF['thinking_len'].mean():.0f} chars\",\n",
        "        'Avg solution length': f\"{SAMPLES_DF['solution_len'].mean():.0f} chars\",\n",
        "        'With test cases': f\"{SAMPLES_DF['has_tests'].sum()} ({SAMPLES_DF['has_tests'].mean()*100:.0f}%)\",\n",
        "    }\n",
        "\n",
        "    for k, v in stats.items():\n",
        "        print(f\"  {k}: {v}\")\n",
        "\n",
        "    print(\"\\nSource distribution:\")\n",
        "    for source, count in SAMPLES_DF['source'].value_counts().head(5).items():\n",
        "        print(f\"  {source}: {count}\")\n",
        "else:\n",
        "    print(\"  ❌ No samples loaded - check dataset structure above\")\n",
        "\n",
        "# ============================================================================\n",
        "# SAMPLE PREVIEW\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Sample Preview (first sample):\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "if SAMPLES:\n",
        "    sample = SAMPLES[0]\n",
        "    print(f\"\\nID: {sample.id}\")\n",
        "    print(f\"Source: {sample.source}\")\n",
        "    print(f\"\\nProblem (first 200 chars):\\n  {sample.problem[:200]}...\")\n",
        "    print(f\"\\nThinking (first 300 chars):\\n  {sample.thinking[:300]}...\")\n",
        "    print(f\"\\nSolution (first 200 chars):\\n  {sample.solution[:200]}...\")\n",
        "else:\n",
        "    print(\"\\n  No samples to preview\")\n",
        "\n",
        "# ============================================================================\n",
        "# EXPORTS\n",
        "# ============================================================================\n",
        "\n",
        "@dataclass\n",
        "class DatasetExports:\n",
        "    \"\"\"Exports from Cell 3.\"\"\"\n",
        "    samples: List[CodeSample]\n",
        "    df: pd.DataFrame\n",
        "    n_samples: int\n",
        "\n",
        "    def get_sample(self, idx: int) -> CodeSample:\n",
        "        return self.samples[idx]\n",
        "\n",
        "    def get_by_id(self, sample_id: str) -> Optional[CodeSample]:\n",
        "        for s in self.samples:\n",
        "            if s.id == sample_id:\n",
        "                return s\n",
        "        return None\n",
        "\n",
        "DATASET = DatasetExports(\n",
        "    samples=SAMPLES,\n",
        "    df=SAMPLES_DF,\n",
        "    n_samples=len(SAMPLES),\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"✅ CELL 3 COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\"\"\n",
        "Exports:\n",
        "  ├── DATASET.samples: List[CodeSample] ({len(SAMPLES)} items)\n",
        "  ├── DATASET.df: DataFrame with metadata\n",
        "  ├── DATASET.get_sample(idx): Get by index\n",
        "  └── DATASET.get_by_id(id): Get by ID\n",
        "\n",
        "CodeSample fields:\n",
        "  ├── .id, .problem, .thinking, .solution, .source\n",
        "  ├── .test_cases (if available)\n",
        "  └── .is_valid, .has_thinking, .has_solution\n",
        "\n",
        "Proceed to Cell 4: CoT Parser\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "8-s3ZJT0_ftl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "CELL 3: LOAD OPENTHOUGHTS DATASET\n",
        "=================================\n",
        "Load DeepSeek-R1 reasoning traces for code generation problems.\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Set, Optional, Tuple\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CELL 3: Load OpenThoughts Dataset\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============================================================================\n",
        "# LOAD DATASET\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[1/4] Loading OpenThoughts-114k...\")\n",
        "\n",
        "ds = load_dataset(\"open-thoughts/OpenThoughts-114k\", split=\"train\")\n",
        "print(f\"  ✅ Loaded {len(ds):,} total samples\")\n",
        "\n",
        "# Inspect structure\n",
        "print(f\"\\n  Dataset columns: {ds.column_names}\")\n",
        "\n",
        "# This dataset uses conversations format!\n",
        "# conversations = [{'from': 'user', 'value': '...'}, {'from': 'assistant', 'value': '...'}]\n",
        "sample_ex = ds[0]\n",
        "print(f\"  Format: conversations list\")\n",
        "if 'conversations' in sample_ex:\n",
        "    convs = sample_ex['conversations']\n",
        "    print(f\"  Conversation turns: {len(convs)}\")\n",
        "    for i, turn in enumerate(convs[:3]):\n",
        "        role = turn.get('from', 'unknown')\n",
        "        val = str(turn.get('value', ''))[:100]\n",
        "        print(f\"    [{i}] {role}: {val}...\")\n",
        "\n",
        "# ============================================================================\n",
        "# PARSE CONVERSATIONS FORMAT\n",
        "# ============================================================================\n",
        "\n",
        "def parse_conversation(example: dict) -> dict:\n",
        "    \"\"\"Extract problem, reasoning, solution from conversations format.\"\"\"\n",
        "    convs = example.get('conversations', [])\n",
        "\n",
        "    problem = \"\"\n",
        "    reasoning = \"\"\n",
        "    solution = \"\"\n",
        "\n",
        "    for turn in convs:\n",
        "        role = turn.get('from', '')\n",
        "        value = turn.get('value', '')\n",
        "\n",
        "        if role == 'user':\n",
        "            problem = value\n",
        "        elif role == 'assistant':\n",
        "            # Assistant response contains both reasoning and solution\n",
        "            # Split on common patterns\n",
        "            full_response = value\n",
        "\n",
        "            # Extract <think>...</think> or similar\n",
        "            think_match = re.search(r'<think>(.*?)</think>', full_response, re.DOTALL)\n",
        "            if think_match:\n",
        "                reasoning = think_match.group(1).strip()\n",
        "                # Solution is everything after </think>\n",
        "                solution = full_response[think_match.end():].strip()\n",
        "            else:\n",
        "                # Try to split on code block\n",
        "                code_match = re.search(r'```(?:python)?\\s*(.*?)```', full_response, re.DOTALL)\n",
        "                if code_match:\n",
        "                    solution = code_match.group(1).strip()\n",
        "                    # Everything before the code block is reasoning\n",
        "                    reasoning = full_response[:code_match.start()].strip()\n",
        "                else:\n",
        "                    # Just use the whole thing as solution\n",
        "                    solution = full_response\n",
        "\n",
        "    return {\n",
        "        'problem': problem,\n",
        "        'reasoning': reasoning,\n",
        "        'solution': solution,\n",
        "    }\n",
        "\n",
        "# ============================================================================\n",
        "# FILTER FOR CODE DOMAIN\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[2/4] Filtering for code problems...\")\n",
        "\n",
        "def is_code_sample(example: dict) -> bool:\n",
        "    \"\"\"Check if sample contains Python code.\"\"\"\n",
        "    parsed = parse_conversation(example)\n",
        "    solution = parsed['solution']\n",
        "\n",
        "    # Check for Python code indicators\n",
        "    code_indicators = ['def ', 'class ', 'import ', 'return ', 'for ', 'while ']\n",
        "    return any(ind in solution for ind in code_indicators)\n",
        "\n",
        "# Filter with progress\n",
        "print(\"  Scanning for code samples...\")\n",
        "code_samples = []\n",
        "for i, ex in enumerate(ds):\n",
        "    if is_code_sample(ex):\n",
        "        code_samples.append(ex)\n",
        "    if i % 20000 == 0:\n",
        "        print(f\"    Scanned {i:,}... found {len(code_samples):,} code samples\")\n",
        "    # Early stop for testing - remove this line for full dataset\n",
        "    # if len(code_samples) >= 500: break\n",
        "\n",
        "print(f\"  ✅ Found {len(code_samples):,} code samples\")\n",
        "\n",
        "# ============================================================================\n",
        "# PARSE AND VALIDATE SAMPLES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[3/4] Parsing samples...\")\n",
        "\n",
        "@dataclass\n",
        "class CodeSample:\n",
        "    \"\"\"A parsed code sample with reasoning trace.\"\"\"\n",
        "    id: str\n",
        "    problem: str\n",
        "    thinking: str\n",
        "    solution: str\n",
        "    source: str\n",
        "    test_cases: Optional[str] = None\n",
        "\n",
        "    @property\n",
        "    def has_thinking(self) -> bool:\n",
        "        return len(self.thinking) > 50\n",
        "\n",
        "    @property\n",
        "    def has_solution(self) -> bool:\n",
        "        return 'def ' in self.solution or 'class ' in self.solution\n",
        "\n",
        "    @property\n",
        "    def is_valid(self) -> bool:\n",
        "        return self.has_thinking and self.has_solution\n",
        "\n",
        "def extract_code(text: str) -> str:\n",
        "    \"\"\"Extract Python code from solution.\"\"\"\n",
        "    # Try ```python block\n",
        "    match = re.search(r'```python\\s*(.*?)```', text, re.DOTALL)\n",
        "    if match:\n",
        "        return match.group(1).strip()\n",
        "\n",
        "    # Try any ``` block\n",
        "    match = re.search(r'```\\s*(.*?)```', text, re.DOTALL)\n",
        "    if match:\n",
        "        code = match.group(1).strip()\n",
        "        if 'def ' in code or 'class ' in code:\n",
        "            return code\n",
        "\n",
        "    # Return raw text if it looks like code\n",
        "    if 'def ' in text or 'class ' in text:\n",
        "        return text.strip()\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "def extract_test_cases(problem: str, solution: str) -> Optional[str]:\n",
        "    \"\"\"Try to extract test cases from problem or solution.\"\"\"\n",
        "    asserts = re.findall(r'assert\\s+.+', solution)\n",
        "    if asserts:\n",
        "        return '\\n'.join(asserts[:5])\n",
        "    return None\n",
        "\n",
        "def parse_sample(idx: int, example: dict) -> CodeSample:\n",
        "    \"\"\"Parse a raw example into CodeSample.\"\"\"\n",
        "    parsed = parse_conversation(example)\n",
        "\n",
        "    problem = parsed['problem']\n",
        "    reasoning = parsed['reasoning']\n",
        "    solution = parsed['solution']\n",
        "\n",
        "    # Clean up solution - extract just the code\n",
        "    code = extract_code(solution) if solution else solution\n",
        "    if not code:\n",
        "        code = solution  # Use raw if extraction fails\n",
        "\n",
        "    return CodeSample(\n",
        "        id=f\"ot_{idx:04d}\",\n",
        "        problem=problem,\n",
        "        thinking=reasoning,\n",
        "        solution=code,\n",
        "        source='OpenThoughts',\n",
        "        test_cases=extract_test_cases(problem, code) if problem and code else None,\n",
        "    )\n",
        "\n",
        "# Parse all code samples\n",
        "parsed_samples = [parse_sample(i, ex) for i, ex in enumerate(code_samples)]\n",
        "valid_samples = [s for s in parsed_samples if s.is_valid]\n",
        "\n",
        "print(f\"  ✅ Parsed {len(parsed_samples):,} samples\")\n",
        "print(f\"  ✅ Valid (has thinking + code): {len(valid_samples):,}\")\n",
        "\n",
        "# If no valid samples, show debug info\n",
        "if len(valid_samples) == 0 and len(parsed_samples) > 0:\n",
        "    print(\"\\n  ⚠️  No valid samples! Debugging first parsed sample:\")\n",
        "    s = parsed_samples[0]\n",
        "    print(f\"    thinking length: {len(s.thinking)}\")\n",
        "    print(f\"    solution length: {len(s.solution)}\")\n",
        "    print(f\"    has_thinking: {s.has_thinking}\")\n",
        "    print(f\"    has_solution: {s.has_solution}\")\n",
        "    if s.thinking:\n",
        "        print(f\"    thinking preview: {s.thinking[:200]}...\")\n",
        "    if s.solution:\n",
        "        print(f\"    solution preview: {s.solution[:200]}...\")\n",
        "\n",
        "# ============================================================================\n",
        "# SELECT FINAL SAMPLE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[4/4] Selecting samples...\")\n",
        "\n",
        "import random\n",
        "random.seed(ENV_CONFIG.RANDOM_SEED)\n",
        "\n",
        "if len(valid_samples) == 0:\n",
        "    print(\"  ❌ No valid samples found!\")\n",
        "    print(\"\\n  Falling back: using samples with ANY code (relaxed validation)\")\n",
        "    # Relax validation - just need some code\n",
        "    valid_samples = [s for s in parsed_samples if len(s.solution) > 20]\n",
        "    print(f\"  ✅ Relaxed: {len(valid_samples)} samples with code\")\n",
        "\n",
        "if len(valid_samples) == 0:\n",
        "    print(\"  ❌ Still no samples! Using raw examples directly...\")\n",
        "    # Last resort: parse from raw conversations\n",
        "    for i, ex in enumerate(code_samples[:200]):\n",
        "        parsed = parse_conversation(ex)\n",
        "        if 'def ' in parsed['solution']:\n",
        "            valid_samples.append(CodeSample(\n",
        "                id=f\"ot_{i:04d}\",\n",
        "                problem=parsed['problem'][:1000],\n",
        "                thinking=parsed['reasoning'],\n",
        "                solution=parsed['solution'],\n",
        "                source='OpenThoughts',\n",
        "            ))\n",
        "    print(f\"  ✅ Direct extraction: {len(valid_samples)} samples\")\n",
        "\n",
        "N_SAMPLES = min(ENV_CONFIG.N_TOTAL, len(valid_samples))\n",
        "SAMPLES = random.sample(valid_samples, N_SAMPLES) if valid_samples else []\n",
        "\n",
        "# Sort by ID for reproducibility\n",
        "SAMPLES.sort(key=lambda x: x.id)\n",
        "\n",
        "print(f\"  ✅ Selected {N_SAMPLES} samples\")\n",
        "\n",
        "# ============================================================================\n",
        "# CREATE DATAFRAME\n",
        "# ============================================================================\n",
        "\n",
        "if SAMPLES:\n",
        "    SAMPLES_DF = pd.DataFrame([\n",
        "        {\n",
        "            'id': s.id,\n",
        "            'problem': s.problem[:500],\n",
        "            'thinking': s.thinking,\n",
        "            'solution': s.solution,\n",
        "            'source': s.source,\n",
        "            'thinking_len': len(s.thinking),\n",
        "            'solution_len': len(s.solution),\n",
        "            'has_tests': s.test_cases is not None,\n",
        "        }\n",
        "        for s in SAMPLES\n",
        "    ])\n",
        "else:\n",
        "    SAMPLES_DF = pd.DataFrame(columns=['id', 'problem', 'thinking', 'solution',\n",
        "                                        'source', 'thinking_len', 'solution_len', 'has_tests'])\n",
        "\n",
        "# ============================================================================\n",
        "# STATISTICS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Dataset Statistics:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "if len(SAMPLES) > 0:\n",
        "    stats = {\n",
        "        'Total samples': len(SAMPLES),\n",
        "        'Avg thinking length': f\"{SAMPLES_DF['thinking_len'].mean():.0f} chars\",\n",
        "        'Avg solution length': f\"{SAMPLES_DF['solution_len'].mean():.0f} chars\",\n",
        "        'With test cases': f\"{SAMPLES_DF['has_tests'].sum()} ({SAMPLES_DF['has_tests'].mean()*100:.0f}%)\",\n",
        "    }\n",
        "\n",
        "    for k, v in stats.items():\n",
        "        print(f\"  {k}: {v}\")\n",
        "\n",
        "    print(\"\\nSource distribution:\")\n",
        "    for source, count in SAMPLES_DF['source'].value_counts().head(5).items():\n",
        "        print(f\"  {source}: {count}\")\n",
        "else:\n",
        "    print(\"  ❌ No samples loaded - check dataset structure above\")\n",
        "\n",
        "# ============================================================================\n",
        "# SAMPLE PREVIEW\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Sample Preview (first sample):\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "if SAMPLES:\n",
        "    sample = SAMPLES[0]\n",
        "    print(f\"\\nID: {sample.id}\")\n",
        "    print(f\"Source: {sample.source}\")\n",
        "    print(f\"\\nProblem (first 200 chars):\\n  {sample.problem[:200]}...\")\n",
        "    print(f\"\\nThinking (first 300 chars):\\n  {sample.thinking[:300]}...\")\n",
        "    print(f\"\\nSolution (first 200 chars):\\n  {sample.solution[:200]}...\")\n",
        "else:\n",
        "    print(\"\\n  No samples to preview\")\n",
        "\n",
        "# ============================================================================\n",
        "# EXPORTS\n",
        "# ============================================================================\n",
        "\n",
        "@dataclass\n",
        "class DatasetExports:\n",
        "    \"\"\"Exports from Cell 3.\"\"\"\n",
        "    samples: List[CodeSample]\n",
        "    df: pd.DataFrame\n",
        "    n_samples: int\n",
        "\n",
        "    def get_sample(self, idx: int) -> CodeSample:\n",
        "        return self.samples[idx]\n",
        "\n",
        "    def get_by_id(self, sample_id: str) -> Optional[CodeSample]:\n",
        "        for s in self.samples:\n",
        "            if s.id == sample_id:\n",
        "                return s\n",
        "        return None\n",
        "\n",
        "DATASET = DatasetExports(\n",
        "    samples=SAMPLES,\n",
        "    df=SAMPLES_DF,\n",
        "    n_samples=len(SAMPLES),\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"✅ CELL 3 COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\"\"\n",
        "Exports:\n",
        "  ├── DATASET.samples: List[CodeSample] ({len(SAMPLES)} items)\n",
        "  ├── DATASET.df: DataFrame with metadata\n",
        "  ├── DATASET.get_sample(idx): Get by index\n",
        "  └── DATASET.get_by_id(id): Get by ID\n",
        "\n",
        "CodeSample fields:\n",
        "  ├── .id, .problem, .thinking, .solution, .source\n",
        "  ├── .test_cases (if available)\n",
        "  └── .is_valid, .has_thinking, .has_solution\n",
        "\n",
        "Proceed to Cell 4: CoT Parser\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "Kbhrt-cNEDUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "CELL 4: COT SEGMENTER\n",
        "=====================\n",
        "Split raw thinking into individual reasoning segments for DFA analysis.\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Set, Tuple\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CELL 4: CoT Segmenter\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============================================================================\n",
        "# DATA STRUCTURES\n",
        "# ============================================================================\n",
        "\n",
        "@dataclass\n",
        "class Segment:\n",
        "    \"\"\"A single reasoning step from CoT.\"\"\"\n",
        "    id: str\n",
        "    text: str\n",
        "    position: int\n",
        "    concepts: Set[str] = field(default_factory=set)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Segment({self.id}, {len(self.text)} chars, {len(self.concepts)} concepts)\"\n",
        "\n",
        "# ============================================================================\n",
        "# SEGMENTATION FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def clean_thinking(text: str) -> str:\n",
        "    \"\"\"Remove thinking tags and clean whitespace.\"\"\"\n",
        "    # Remove various thinking tags\n",
        "    patterns = [\n",
        "        r'<\\|begin_of_thought\\|>',\n",
        "        r'<\\|end_of_thought\\|>',\n",
        "        r'<think>',\n",
        "        r'</think>',\n",
        "        r'<reasoning>',\n",
        "        r'</reasoning>',\n",
        "    ]\n",
        "    for p in patterns:\n",
        "        text = re.sub(p, '', text, flags=re.IGNORECASE)\n",
        "    return text.strip()\n",
        "\n",
        "def split_into_sentences(text: str) -> List[str]:\n",
        "    \"\"\"Split text into sentences, handling code blocks.\"\"\"\n",
        "    # Protect code blocks\n",
        "    code_blocks = []\n",
        "    def save_code(m):\n",
        "        code_blocks.append(m.group(0))\n",
        "        return f\"__CODE_BLOCK_{len(code_blocks)-1}__\"\n",
        "\n",
        "    text = re.sub(r'```.*?```', save_code, text, flags=re.DOTALL)\n",
        "    text = re.sub(r'`[^`]+`', save_code, text)\n",
        "\n",
        "    # Split on sentence boundaries\n",
        "    # Handle: . ! ? followed by space and capital, or newline\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z])|(?<=\\n)\\s*(?=\\w)', text)\n",
        "\n",
        "    # Restore code blocks\n",
        "    result = []\n",
        "    for s in sentences:\n",
        "        for i, block in enumerate(code_blocks):\n",
        "            s = s.replace(f\"__CODE_BLOCK_{i}__\", block)\n",
        "        s = s.strip()\n",
        "        if s:\n",
        "            result.append(s)\n",
        "\n",
        "    return result\n",
        "\n",
        "def split_by_markers(text: str) -> List[str]:\n",
        "    \"\"\"Split by explicit step markers (1. 2. 3. or - or *).\"\"\"\n",
        "    # Try numbered steps: \"1.\" \"2.\" etc\n",
        "    numbered = re.split(r'\\n\\s*\\d+[.)]\\s+', text)\n",
        "    if len(numbered) > 2:\n",
        "        return [s.strip() for s in numbered if s.strip()]\n",
        "\n",
        "    # Try bullet points\n",
        "    bullets = re.split(r'\\n\\s*[-*•]\\s+', text)\n",
        "    if len(bullets) > 2:\n",
        "        return [s.strip() for s in bullets if s.strip()]\n",
        "\n",
        "    # Try paragraph breaks (double newline)\n",
        "    paragraphs = re.split(r'\\n\\s*\\n', text)\n",
        "    if len(paragraphs) > 1:\n",
        "        return [s.strip() for s in paragraphs if s.strip()]\n",
        "\n",
        "    return []\n",
        "\n",
        "def segment_cot(thinking: str, min_length: int = 20) -> List[Segment]:\n",
        "    \"\"\"\n",
        "    Segment CoT into reasoning steps.\n",
        "\n",
        "    Strategy:\n",
        "    1. Try explicit markers (1. 2. 3. or bullets)\n",
        "    2. Fall back to sentence splitting\n",
        "    3. Merge very short segments\n",
        "    \"\"\"\n",
        "    cleaned = clean_thinking(thinking)\n",
        "\n",
        "    if not cleaned:\n",
        "        return []\n",
        "\n",
        "    # Try marker-based splitting first\n",
        "    parts = split_by_markers(cleaned)\n",
        "\n",
        "    # Fall back to sentence splitting\n",
        "    if len(parts) < 3:\n",
        "        parts = split_into_sentences(cleaned)\n",
        "\n",
        "    # Filter short segments and create Segment objects\n",
        "    segments = []\n",
        "    for i, text in enumerate(parts):\n",
        "        if len(text) >= min_length:\n",
        "            segments.append(Segment(\n",
        "                id=f\"s{i}\",\n",
        "                text=text,\n",
        "                position=i,\n",
        "                concepts=set(),  # Filled in Cell 5\n",
        "            ))\n",
        "\n",
        "    # Merge consecutive short segments if we have too few\n",
        "    if len(segments) < 3 and len(cleaned) > 200:\n",
        "        # Just chunk by ~200 chars\n",
        "        chunks = [cleaned[i:i+200] for i in range(0, len(cleaned), 200)]\n",
        "        segments = [\n",
        "            Segment(id=f\"s{i}\", text=chunk.strip(), position=i, concepts=set())\n",
        "            for i, chunk in enumerate(chunks) if len(chunk.strip()) >= min_length\n",
        "        ]\n",
        "\n",
        "    return segments\n",
        "\n",
        "# ============================================================================\n",
        "# BATCH PROCESSING\n",
        "# ============================================================================\n",
        "\n",
        "def segment_all_samples(samples: list) -> dict:\n",
        "    \"\"\"Segment all samples, return dict mapping id -> segments.\"\"\"\n",
        "    results = {}\n",
        "    stats = {'total': 0, 'min': float('inf'), 'max': 0, 'sum': 0}\n",
        "\n",
        "    for sample in samples:\n",
        "        segments = segment_cot(sample.thinking)\n",
        "        results[sample.id] = segments\n",
        "\n",
        "        n = len(segments)\n",
        "        stats['total'] += 1\n",
        "        stats['sum'] += n\n",
        "        stats['min'] = min(stats['min'], n)\n",
        "        stats['max'] = max(stats['max'], n)\n",
        "\n",
        "    stats['avg'] = stats['sum'] / stats['total'] if stats['total'] > 0 else 0\n",
        "    return results, stats\n",
        "\n",
        "# ============================================================================\n",
        "# PROCESS DATASET\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[1/2] Segmenting CoT traces...\")\n",
        "\n",
        "SEGMENTED, seg_stats = segment_all_samples(DATASET.samples)\n",
        "\n",
        "print(f\"  ✅ Segmented {seg_stats['total']} samples\")\n",
        "print(f\"  ✅ Segments per sample: min={seg_stats['min']}, avg={seg_stats['avg']:.1f}, max={seg_stats['max']}\")\n",
        "\n",
        "# ============================================================================\n",
        "# PREVIEW\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[2/2] Preview...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Show first sample's segments\n",
        "sample = DATASET.samples[0]\n",
        "segments = SEGMENTED[sample.id]\n",
        "\n",
        "print(f\"Sample: {sample.id}\")\n",
        "print(f\"Total segments: {len(segments)}\")\n",
        "print(f\"\\nFirst 3 segments:\")\n",
        "\n",
        "for seg in segments[:3]:\n",
        "    preview = seg.text[:100].replace('\\n', ' ')\n",
        "    print(f\"\\n  [{seg.id}] ({len(seg.text)} chars)\")\n",
        "    print(f\"      {preview}...\")\n",
        "\n",
        "# ============================================================================\n",
        "# EXPORTS\n",
        "# ============================================================================\n",
        "\n",
        "@dataclass\n",
        "class SegmenterExports:\n",
        "    \"\"\"Exports from Cell 4.\"\"\"\n",
        "    segmented: dict  # sample_id -> List[Segment]\n",
        "    segment_cot: callable\n",
        "    stats: dict\n",
        "\n",
        "SEGMENTER = SegmenterExports(\n",
        "    segmented=SEGMENTED,\n",
        "    segment_cot=segment_cot,\n",
        "    stats=seg_stats,\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"✅ CELL 4 COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\"\"\n",
        "Stats:\n",
        "  ├── Samples processed: {seg_stats['total']}\n",
        "  ├── Avg segments/sample: {seg_stats['avg']:.1f}\n",
        "  ├── Total segments: {seg_stats['sum']}\n",
        "  └── Segment length filter: ≥20 chars\n",
        "\n",
        "Exports:\n",
        "  ├── SEGMENTER.segmented[sample_id] → List[Segment]\n",
        "  ├── SEGMENTER.segment_cot(text) → List[Segment]\n",
        "  └── SEGMENTER.stats\n",
        "\n",
        "Segment fields:\n",
        "  ├── .id (\"s0\", \"s1\", ...)\n",
        "  ├── .text (raw reasoning text)\n",
        "  ├── .position (order in CoT)\n",
        "  └── .concepts (empty, filled in Cell 5)\n",
        "\n",
        "Proceed to Cell 5: Concept Extraction\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "YVXFzkuBFlmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "CELL 5: CONCEPT EXTRACTION\n",
        "==========================\n",
        "Extract programming concepts from CoT segments and code for reaching definitions.\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "import ast\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Set, Dict, Tuple, Optional\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CELL 5: Concept Extraction\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============================================================================\n",
        "# CONCEPT VOCABULARY (22 PROGRAMMING CONCEPTS)\n",
        "# ============================================================================\n",
        "\n",
        "CONCEPT_VOCABULARY: Dict[str, Set[str]] = {\n",
        "    # Data Structures (8)\n",
        "    'dict': {'hash', 'map', 'dictionary', 'hashmap', 'hash map', 'key-value',\n",
        "             'lookup table', 'mapping', 'counter', 'dict', '{}'},\n",
        "    'list': {'array', 'list', 'sequence', 'collection', 'elements', 'items', '[]'},\n",
        "    'set': {'set', 'unique', 'deduplicate', 'distinct', 'set()'},\n",
        "    'stack': {'stack', 'lifo', 'push', 'pop', 'append and pop'},\n",
        "    'queue': {'queue', 'fifo', 'deque', 'bfs', 'collections.deque'},\n",
        "    'heap': {'heap', 'priority queue', 'heapq', 'heappush', 'heappop', 'min heap', 'max heap'},\n",
        "    'tree': {'tree', 'binary tree', 'bst', 'trie', 'node', 'root', 'left child', 'right child'},\n",
        "    'graph': {'graph', 'vertices', 'edges', 'adjacent', 'neighbor', 'dfs', 'adjacency'},\n",
        "\n",
        "    # Algorithms (7)\n",
        "    'sort': {'sort', 'order', 'arrange', 'sorted', 'ascending', 'descending', 'sorted()'},\n",
        "    'search': {'search', 'find', 'lookup', 'binary search', 'locate', 'bisect'},\n",
        "    'recursion': {'recursive', 'recursion', 'base case', 'call itself', 'recur'},\n",
        "    'dp': {'dynamic programming', 'memoization', 'memo', 'dp', 'subproblem', 'cache', 'lru_cache'},\n",
        "    'greedy': {'greedy', 'local optimal', 'best choice', 'optimal substructure'},\n",
        "    'two_pointer': {'two pointer', 'left right', 'start end', 'sliding window', 'window'},\n",
        "    'backtrack': {'backtrack', 'prune', 'explore', 'candidates', 'backtracking'},\n",
        "\n",
        "    # Control Flow (3)\n",
        "    'loop': {'iterate', 'loop', 'for each', 'traverse', 'go through', 'while', 'for'},\n",
        "    'condition': {'if', 'check', 'condition', 'edge case', 'boundary', 'elif', 'else'},\n",
        "    'early_return': {'return early', 'base case', 'edge case', 'special case', 'return'},\n",
        "\n",
        "    # Operations (4)\n",
        "    'count': {'count', 'frequency', 'occurrences', 'how many', 'counter'},\n",
        "    'sum': {'sum', 'total', 'add up', 'accumulate', 'sum()'},\n",
        "    'max_min': {'maximum', 'minimum', 'max', 'min', 'largest', 'smallest', 'max()', 'min()'},\n",
        "    'string_op': {'string', 'character', 'substring', 'split', 'join', 'strip', 'str'},\n",
        "}\n",
        "\n",
        "# Reverse mapping: keyword -> concept\n",
        "KEYWORD_TO_CONCEPT: Dict[str, str] = {}\n",
        "for concept, keywords in CONCEPT_VOCABULARY.items():\n",
        "    for kw in keywords:\n",
        "        KEYWORD_TO_CONCEPT[kw.lower()] = concept\n",
        "\n",
        "# ============================================================================\n",
        "# COT CONCEPT EXTRACTION\n",
        "# ============================================================================\n",
        "\n",
        "def extract_cot_concepts(text: str) -> Set[str]:\n",
        "    \"\"\"Extract programming concepts from CoT text.\"\"\"\n",
        "    text_lower = text.lower()\n",
        "    found = set()\n",
        "\n",
        "    # Check each keyword\n",
        "    for keyword, concept in KEYWORD_TO_CONCEPT.items():\n",
        "        # Use word boundary for short keywords to avoid false positives\n",
        "        if len(keyword) <= 3:\n",
        "            if re.search(rf'\\b{re.escape(keyword)}\\b', text_lower):\n",
        "                found.add(concept)\n",
        "        else:\n",
        "            if keyword in text_lower:\n",
        "                found.add(concept)\n",
        "\n",
        "    return found\n",
        "\n",
        "# ============================================================================\n",
        "# AST NODE TO CONCEPT MAPPING\n",
        "# ============================================================================\n",
        "\n",
        "AST_TO_CONCEPT: Dict[str, str] = {\n",
        "    # Data structures\n",
        "    'Dict': 'dict',\n",
        "    'DictComp': 'dict',\n",
        "    'List': 'list',\n",
        "    'ListComp': 'list',\n",
        "    'Set': 'set',\n",
        "    'SetComp': 'set',\n",
        "    'Tuple': 'list',  # Treat tuple as list-like\n",
        "\n",
        "    # Control flow\n",
        "    'For': 'loop',\n",
        "    'While': 'loop',\n",
        "    'AsyncFor': 'loop',\n",
        "    'If': 'condition',\n",
        "    'IfExp': 'condition',\n",
        "    'Return': 'early_return',\n",
        "\n",
        "    # Comprehensions indicate loops\n",
        "    'comprehension': 'loop',\n",
        "}\n",
        "\n",
        "# Function calls to concepts\n",
        "CALL_TO_CONCEPT: Dict[str, str] = {\n",
        "    # Built-ins\n",
        "    'sorted': 'sort',\n",
        "    'sort': 'sort',\n",
        "    'max': 'max_min',\n",
        "    'min': 'max_min',\n",
        "    'sum': 'sum',\n",
        "    'len': 'count',\n",
        "    'count': 'count',\n",
        "    'range': 'loop',\n",
        "    'enumerate': 'loop',\n",
        "    'zip': 'loop',\n",
        "    'map': 'loop',\n",
        "    'filter': 'loop',\n",
        "\n",
        "    # Collections\n",
        "    'dict': 'dict',\n",
        "    'list': 'list',\n",
        "    'set': 'set',\n",
        "    'deque': 'queue',\n",
        "    'Counter': 'dict',\n",
        "    'defaultdict': 'dict',\n",
        "    'OrderedDict': 'dict',\n",
        "\n",
        "    # Heap\n",
        "    'heappush': 'heap',\n",
        "    'heappop': 'heap',\n",
        "    'heapify': 'heap',\n",
        "    'heapreplace': 'heap',\n",
        "\n",
        "    # Search\n",
        "    'bisect': 'search',\n",
        "    'bisect_left': 'search',\n",
        "    'bisect_right': 'search',\n",
        "    'index': 'search',\n",
        "    'find': 'search',\n",
        "\n",
        "    # String\n",
        "    'split': 'string_op',\n",
        "    'join': 'string_op',\n",
        "    'strip': 'string_op',\n",
        "    'replace': 'string_op',\n",
        "    'lower': 'string_op',\n",
        "    'upper': 'string_op',\n",
        "\n",
        "    # DP/Memoization\n",
        "    'lru_cache': 'dp',\n",
        "    'cache': 'dp',\n",
        "}\n",
        "\n",
        "# ============================================================================\n",
        "# CODE ELEMENT DATA STRUCTURE\n",
        "# ============================================================================\n",
        "\n",
        "@dataclass\n",
        "class CodeElement:\n",
        "    \"\"\"A code construct that may be linked to CoT reasoning.\"\"\"\n",
        "    id: str\n",
        "    node_type: str\n",
        "    line_number: int\n",
        "    concepts: Set[str] = field(default_factory=set)\n",
        "    source_text: str = \"\"\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"CodeElement({self.id}, {self.node_type}, line {self.line_number}, {self.concepts})\"\n",
        "\n",
        "# ============================================================================\n",
        "# AST CONCEPT EXTRACTION\n",
        "# ============================================================================\n",
        "\n",
        "class ConceptVisitor(ast.NodeVisitor):\n",
        "    \"\"\"AST visitor that extracts concepts from code.\"\"\"\n",
        "\n",
        "    def __init__(self, source_lines: List[str]):\n",
        "        self.elements: List[CodeElement] = []\n",
        "        self.concepts: Set[str] = set()\n",
        "        self.source_lines = source_lines\n",
        "        self.element_counter = 0\n",
        "\n",
        "    def _add_element(self, node: ast.AST, node_type: str, concepts: Set[str]):\n",
        "        \"\"\"Create and store a CodeElement.\"\"\"\n",
        "        line = getattr(node, 'lineno', 0)\n",
        "        source = self.source_lines[line-1].strip() if 0 < line <= len(self.source_lines) else \"\"\n",
        "\n",
        "        elem = CodeElement(\n",
        "            id=f\"c{self.element_counter}\",\n",
        "            node_type=node_type,\n",
        "            line_number=line,\n",
        "            concepts=concepts,\n",
        "            source_text=source[:100],\n",
        "        )\n",
        "        self.elements.append(elem)\n",
        "        self.concepts.update(concepts)\n",
        "        self.element_counter += 1\n",
        "\n",
        "    def visit_Dict(self, node):\n",
        "        self._add_element(node, 'Dict', {'dict'})\n",
        "        self.generic_visit(node)\n",
        "\n",
        "    def visit_DictComp(self, node):\n",
        "        self._add_element(node, 'DictComp', {'dict', 'loop'})\n",
        "        self.generic_visit(node)\n",
        "\n",
        "    def visit_List(self, node):\n",
        "        self._add_element(node, 'List', {'list'})\n",
        "        self.generic_visit(node)\n",
        "\n",
        "    def visit_ListComp(self, node):\n",
        "        self._add_element(node, 'ListComp', {'list', 'loop'})\n",
        "        self.generic_visit(node)\n",
        "\n",
        "    def visit_Set(self, node):\n",
        "        self._add_element(node, 'Set', {'set'})\n",
        "        self.generic_visit(node)\n",
        "\n",
        "    def visit_SetComp(self, node):\n",
        "        self._add_element(node, 'SetComp', {'set', 'loop'})\n",
        "        self.generic_visit(node)\n",
        "\n",
        "    def visit_For(self, node):\n",
        "        self._add_element(node, 'For', {'loop'})\n",
        "        self.generic_visit(node)\n",
        "\n",
        "    def visit_While(self, node):\n",
        "        self._add_element(node, 'While', {'loop'})\n",
        "        self.generic_visit(node)\n",
        "\n",
        "    def visit_If(self, node):\n",
        "        self._add_element(node, 'If', {'condition'})\n",
        "        self.generic_visit(node)\n",
        "\n",
        "    def visit_Return(self, node):\n",
        "        self._add_element(node, 'Return', {'early_return'})\n",
        "        self.generic_visit(node)\n",
        "\n",
        "    def visit_Call(self, node):\n",
        "        \"\"\"Handle function calls.\"\"\"\n",
        "        func_name = None\n",
        "\n",
        "        # Get function name\n",
        "        if isinstance(node.func, ast.Name):\n",
        "            func_name = node.func.id\n",
        "        elif isinstance(node.func, ast.Attribute):\n",
        "            func_name = node.func.attr\n",
        "\n",
        "        if func_name and func_name in CALL_TO_CONCEPT:\n",
        "            concept = CALL_TO_CONCEPT[func_name]\n",
        "            self._add_element(node, f'Call:{func_name}', {concept})\n",
        "\n",
        "        self.generic_visit(node)\n",
        "\n",
        "    def visit_FunctionDef(self, node):\n",
        "        \"\"\"Check for recursive calls.\"\"\"\n",
        "        # Look for self-calls (recursion)\n",
        "        for child in ast.walk(node):\n",
        "            if isinstance(child, ast.Call):\n",
        "                if isinstance(child.func, ast.Name) and child.func.id == node.name:\n",
        "                    self._add_element(node, 'Recursion', {'recursion'})\n",
        "                    break\n",
        "        self.generic_visit(node)\n",
        "\n",
        "def extract_code_concepts(code: str) -> Tuple[Set[str], List[CodeElement]]:\n",
        "    \"\"\"\n",
        "    Extract concepts from Python code using AST analysis.\n",
        "\n",
        "    Returns:\n",
        "        (all_concepts, list_of_code_elements)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        tree = ast.parse(code)\n",
        "    except SyntaxError:\n",
        "        return set(), []\n",
        "\n",
        "    source_lines = code.split('\\n')\n",
        "    visitor = ConceptVisitor(source_lines)\n",
        "    visitor.visit(tree)\n",
        "\n",
        "    return visitor.concepts, visitor.elements\n",
        "\n",
        "# ============================================================================\n",
        "# UPDATE SEGMENTS WITH CONCEPTS\n",
        "# ============================================================================\n",
        "\n",
        "def enrich_segments(segments: List[Segment]) -> List[Segment]:\n",
        "    \"\"\"Add concepts to each segment.\"\"\"\n",
        "    for seg in segments:\n",
        "        seg.concepts = extract_cot_concepts(seg.text)\n",
        "    return segments\n",
        "\n",
        "# ============================================================================\n",
        "# TESTS\n",
        "# ============================================================================\n",
        "\n",
        "def run_tests():\n",
        "    \"\"\"Validate concept extraction.\"\"\"\n",
        "    print(\"\\n[TESTS] Running validation...\")\n",
        "    results = []\n",
        "\n",
        "    # Test 1: CoT hash map/sort\n",
        "    c = extract_cot_concepts(\"I'll use a hash map for O(1) lookup, then sort the results\")\n",
        "    results.append(('dict' in c and 'sort' in c, \"CoT: hash map→dict, sort→sort\"))\n",
        "\n",
        "    # Test 2: CoT loop/array\n",
        "    c = extract_cot_concepts(\"I'll iterate through each element in the array\")\n",
        "    results.append(('loop' in c and 'list' in c, \"CoT: iterate→loop, array→list\"))\n",
        "\n",
        "    # Test 3: AST dict\n",
        "    c, _ = extract_code_concepts(\"seen = {}\")\n",
        "    results.append(('dict' in c, \"AST: {}→dict\"))\n",
        "\n",
        "    # Test 4: AST for loop\n",
        "    c, _ = extract_code_concepts(\"for i in range(10):\\n    print(i)\")\n",
        "    results.append(('loop' in c, \"AST: for→loop\"))\n",
        "\n",
        "    # Test 5: AST sorted\n",
        "    c, _ = extract_code_concepts(\"result = sorted(nums)\")\n",
        "    results.append(('sort' in c, \"AST: sorted()→sort\"))\n",
        "\n",
        "    # Test 6: AST heap\n",
        "    c, _ = extract_code_concepts(\"import heapq\\nheapq.heappush(h, item)\")\n",
        "    results.append(('heap' in c, \"AST: heappush→heap\"))\n",
        "\n",
        "    # Test 7: AST recursion\n",
        "    c, _ = extract_code_concepts(\"def fib(n):\\n    if n <= 1: return n\\n    return fib(n-1) + fib(n-2)\")\n",
        "    results.append(('recursion' in c, \"AST: self-call→recursion\"))\n",
        "\n",
        "    # Test 8: Multiple concepts\n",
        "    code = \"def solve(nums):\\n    seen = {}\\n    for n in nums:\\n        if n in seen: return True\\n        seen[n] = True\\n    return False\"\n",
        "    c, _ = extract_code_concepts(code)\n",
        "    results.append(({'dict','loop','condition','early_return'}.issubset(c), \"AST: multi-concept\"))\n",
        "\n",
        "    # Test 9: Invalid code\n",
        "    c, e = extract_code_concepts(\"this is not valid python {{{{\")\n",
        "    results.append((c == set() and e == [], \"Invalid code→empty\"))\n",
        "\n",
        "    # Test 10: DP in CoT\n",
        "    c = extract_cot_concepts(\"use dynamic programming with memoization to cache results\")\n",
        "    results.append(('dp' in c, \"CoT: DP/memoization→dp\"))\n",
        "\n",
        "    passed = sum(1 for r, _ in results if r)\n",
        "    for ok, desc in results:\n",
        "        print(f\"  {'✅' if ok else '❌'} {desc}\")\n",
        "    print(f\"\\n  Results: {passed}/{len(results)} tests passed\")\n",
        "    return passed, len(results) - passed\n",
        "\n",
        "# Run tests\n",
        "test_passed, test_failed = run_tests()\n",
        "\n",
        "# ============================================================================\n",
        "# PROCESS DATASET\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"[1/3] Extracting concepts from CoT segments...\")\n",
        "\n",
        "cot_concept_stats = {'total_segments': 0, 'with_concepts': 0, 'concept_counts': {}}\n",
        "\n",
        "for sample_id, segments in SEGMENTER.segmented.items():\n",
        "    enriched = enrich_segments(segments)\n",
        "    SEGMENTER.segmented[sample_id] = enriched\n",
        "\n",
        "    for seg in enriched:\n",
        "        cot_concept_stats['total_segments'] += 1\n",
        "        if seg.concepts:\n",
        "            cot_concept_stats['with_concepts'] += 1\n",
        "        for c in seg.concepts:\n",
        "            cot_concept_stats['concept_counts'][c] = cot_concept_stats['concept_counts'].get(c, 0) + 1\n",
        "\n",
        "print(f\"  ✅ Processed {cot_concept_stats['total_segments']} segments\")\n",
        "print(f\"  ✅ Segments with concepts: {cot_concept_stats['with_concepts']} ({100*cot_concept_stats['with_concepts']/max(1,cot_concept_stats['total_segments']):.0f}%)\")\n",
        "\n",
        "# ============================================================================\n",
        "# EXTRACT CODE CONCEPTS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[2/3] Extracting concepts from code...\")\n",
        "\n",
        "CODE_ANALYSIS: Dict[str, Tuple[Set[str], List[CodeElement]]] = {}\n",
        "code_concept_stats = {'total_elements': 0, 'concept_counts': {}}\n",
        "\n",
        "for sample in DATASET.samples:\n",
        "    concepts, elements = extract_code_concepts(sample.solution)\n",
        "    CODE_ANALYSIS[sample.id] = (concepts, elements)\n",
        "\n",
        "    code_concept_stats['total_elements'] += len(elements)\n",
        "    for c in concepts:\n",
        "        code_concept_stats['concept_counts'][c] = code_concept_stats['concept_counts'].get(c, 0) + 1\n",
        "\n",
        "print(f\"  ✅ Processed {len(CODE_ANALYSIS)} samples\")\n",
        "print(f\"  ✅ Total code elements: {code_concept_stats['total_elements']}\")\n",
        "\n",
        "# ============================================================================\n",
        "# CONCEPT DISTRIBUTION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[3/3] Concept distribution...\")\n",
        "\n",
        "top_cot = sorted(cot_concept_stats['concept_counts'].items(), key=lambda x: -x[1])[:5]\n",
        "top_code = sorted(code_concept_stats['concept_counts'].items(), key=lambda x: -x[1])[:5]\n",
        "\n",
        "print(f\"  Top CoT: {', '.join(f'{c}:{n}' for c,n in top_cot)}\")\n",
        "print(f\"  Top Code: {', '.join(f'{c}:{n}' for c,n in top_code)}\")\n",
        "\n",
        "# ============================================================================\n",
        "# PREVIEW\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "sample = DATASET.samples[0]\n",
        "segments = SEGMENTER.segmented[sample.id]\n",
        "code_concepts, code_elements = CODE_ANALYSIS[sample.id]\n",
        "\n",
        "print(f\"Preview ({sample.id}): {len(segments)} segs, {len(code_elements)} code elems\")\n",
        "print(f\"  CoT concepts: {[s.concepts for s in segments[:3] if s.concepts]}\")\n",
        "print(f\"  Code concepts: {code_concepts}\")\n",
        "\n",
        "# ============================================================================\n",
        "# EXPORTS\n",
        "# ============================================================================\n",
        "\n",
        "@dataclass\n",
        "class ConceptExports:\n",
        "    \"\"\"Exports from Cell 5.\"\"\"\n",
        "    vocabulary: Dict[str, Set[str]]\n",
        "    extract_cot: callable\n",
        "    extract_code: callable\n",
        "    code_analysis: Dict[str, Tuple[Set[str], List[CodeElement]]]\n",
        "    cot_stats: dict\n",
        "    code_stats: dict\n",
        "\n",
        "CONCEPTS = ConceptExports(\n",
        "    vocabulary=CONCEPT_VOCABULARY,\n",
        "    extract_cot=extract_cot_concepts,\n",
        "    extract_code=extract_code_concepts,\n",
        "    code_analysis=CODE_ANALYSIS,\n",
        "    cot_stats=cot_concept_stats,\n",
        "    code_stats=code_concept_stats,\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"✅ CELL 5 COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\"\"\n",
        "Tests: {test_passed}/{test_passed+test_failed} passed\n",
        "\n",
        "Stats:\n",
        "  ├── CoT segments: {cot_concept_stats['total_segments']}\n",
        "  ├── Segments with concepts: {cot_concept_stats['with_concepts']}\n",
        "  ├── Code elements: {code_concept_stats['total_elements']}\n",
        "  └── Concept vocabulary: {len(CONCEPT_VOCABULARY)} concepts\n",
        "\n",
        "Exports:\n",
        "  ├── CONCEPTS.vocabulary → Dict[concept, keywords]\n",
        "  ├── CONCEPTS.extract_cot(text) → Set[str]\n",
        "  ├── CONCEPTS.extract_code(code) → (Set, List[CodeElement])\n",
        "  ├── CONCEPTS.code_analysis[sample_id] → (concepts, elements)\n",
        "  └── SEGMENTER.segmented[id] now has concepts filled\n",
        "\n",
        "Proceed to Cell 6: Reaching Definitions\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "EMuzHSU4K_K3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}